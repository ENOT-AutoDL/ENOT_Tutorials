{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with ENOT\n",
    "\n",
    "This notebook describes the basic steps you need to optimize an architecture using ENOT framework.\n",
    "\n",
    "### Main chapters of this notebook:\n",
    "1. Setup the environment\n",
    "1. Prepare dataset and create dataloaders\n",
    "1. Create model and move it into search space\n",
    "1. Pretrain constructed search space\n",
    "1. Search best architecture\n",
    "1. Tune model with the best architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the environment\n",
    "First, let's set up the environment and make some common imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "# You may need to change this variable to match free GPU index\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch_optimizer import RAdam\n",
    "\n",
    "from enot.models import SearchSpaceModel\n",
    "from enot.models.mobilenet import build_mobilenet\n",
    "from enot.optimize import EnotPretrainOptimizer\n",
    "from enot.optimize import EnotSearchOptimizer\n",
    "from enot.latency import initialize_latency\n",
    "from enot.latency import min_latency\n",
    "from enot.latency import max_latency\n",
    "from enot.latency import mean_latency\n",
    "from enot.latency import best_arch_latency\n",
    "\n",
    "from tutorial_utils.train_utils import accuracy\n",
    "from tutorial_utils.train_utils import WarmupScheduler\n",
    "\n",
    "from tutorial_utils.checkpoints import download_getting_started_pretrain_checkpoint\n",
    "from tutorial_utils.dataset import create_imagenette_dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the following cell we setup all necessary dirs\n",
    "\n",
    "* `ENOT_HOME_DIR` - ENOT framework home directory\n",
    "* `ENOT_DATASETS_DIR` - root directory for datasets (imagenette2, ...)\n",
    "* `PROJECT_DIR` - project directory to save training logs, checkpoints, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENOT_HOME_DIR = Path.home() / '.enot'\n",
    "ENOT_DATASETS_DIR = ENOT_HOME_DIR / 'datasets'\n",
    "PROJECT_DIR = ENOT_HOME_DIR / 'getting_started'\n",
    "\n",
    "ENOT_HOME_DIR.mkdir(exist_ok=True)\n",
    "ENOT_DATASETS_DIR.mkdir(exist_ok=True)\n",
    "PROJECT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset and create dataloaders\n",
    "\n",
    "We will use dataset Imagenette2 in this example. <br>\n",
    "`create_imagenette_dataloaders` function prepares datasets for you in this example; specifically, it:\n",
    "1. downloads and unpacks dataset into `ENOT_DATASETS_DIR`;\n",
    "1. splits dataset into 4 parts, and saves annotations of every part in `PROJECT_DIR`;\n",
    "1. creates dataloaders for every stage, and returns dataloaders as a dictionary.\n",
    "\n",
    "The four parts of the dataset:\n",
    "* train: for pretrain and tuning stages (`PROJECT_DIR`/train.csv)\n",
    "* validation: to choose checkpoint for architecture optimization (`PROJECT_DIR`/validation.csv)\n",
    "* search: for architecture search stage (`PROJECT_DIR`/search.csv)\n",
    "* test: hold-out data for testing (`PROJECT_DIR`/test.csv)\n",
    "\n",
    "Available dataloaders in the dictionary:\n",
    "* pretrain_train_dataloader:\n",
    "    training dataloader for \"pretrain\" stage (using data from `PROJECT_DIR`/train.csv)\n",
    "\n",
    "* pretrain_validation_dataloader:\n",
    "    validation dataloader for \"pretrain\" stage (using data from `PROJECT_DIR`/validation.csv)\n",
    "\n",
    "* search_train_dataloader:\n",
    "    training dataloader for \"search\" stage (using data from `PROJECT_DIR`/search.csv)\n",
    "\n",
    "* search_validation_dataloader:\n",
    "    validation dataloader for \"search\" stage (using data from `PROJECT_DIR`/search.csv)\n",
    "\n",
    "* tune_train_dataloader:\n",
    "    training dataloader for \"finetune\" stage (same as pretrain_train_dataloader)\n",
    "\n",
    "* tune_validation_dataloader:\n",
    "    validation dataloader for \"finetune\" stage (same as pretrain_validation_dataloader)\n",
    "\n",
    "**NOTE:**<br>\n",
    "CSV annotations use the following format:\n",
    "```\n",
    "filepath,label\n",
    "<relative_path_1>,<int_label_1>\n",
    "<relative_path_2>,<int_label_2>\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = create_imagenette_dataloaders(\n",
    "    dataset_root_dir=ENOT_DATASETS_DIR, \n",
    "    project_dir=PROJECT_DIR,\n",
    "    input_size=(224, 224),\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model and move it into search space\n",
    "\n",
    "Our architecture optimization procedure selects the best combination of operations from a user-defined search space. The easiest way to define one is to take a base architecture and add similar operations with different parameters: kernel size, expansion ratio, activation, etc. You can also add different kinds of operations or implement your own (see \"Tutorial - adding a custom operation\").\n",
    "\n",
    "In this example, we took MobileNet v2 as a base architecture and made a search space from MobileNet inverted bottleneck blocks.\n",
    "\n",
    "First, let's define a model for an image classification task. MobileNet-like models can be built by `build_mobilenet` function. `build_mobilenet` function returns a model with the following structure: (inputs) -> stem -> blocks -> head -> (outputs). Stem and head have a fixed structure, while blocks consist of user-defined operations. The template class will follow MobileNet v2 structure, which can be found in [torchvision library](https://github.com/pytorch/vision/blob/master/torchvision/models/mobilenetv2.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search space will have these ops as choose options in each layer.\n",
    "# Short format for operations is 'Name_param1=value1_param2=value2...'.\n",
    "# MIB is a MNv2 inverted bottleneck, k is a kernel size for depthwise\n",
    "# convolution, and t is an expansion ratio coefficient.\n",
    "# See more in-depth info in \"Tutorial - adding custom operations\".\n",
    "SEARCH_OPS = [\n",
    "    'MIB_k=3_t=6',\n",
    "    'MIB_k=5_t=6',\n",
    "    'MIB_k=7_t=6',\n",
    "]\n",
    "\n",
    "# build model\n",
    "model = build_mobilenet(\n",
    "    search_ops=SEARCH_OPS,\n",
    "    num_classes=10,\n",
    "    blocks_out_channels=[24, 32, 64, 96, 160, 320],\n",
    "    blocks_count=[2, 2, 2, 1, 2, 1],\n",
    "    blocks_stride=[2, 2, 2, 1, 2, 1],\n",
    ")\n",
    "\n",
    "# move model to search space\n",
    "search_space = SearchSpaceModel(model).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain constructed search space\n",
    "\n",
    "\"Pretrain\" phase is the first phase of NAS procedure. In \"pretrain\" phase, we train user-defined search space with different architecture combinations to further compare their task performance in search phase. \n",
    "\n",
    "We offer `EnotPretrainOptimizer` class (a wrapper for regular pytorch optimizer), which does all necessary \"pretrain\" magic. You should use `search_space` as the model and `EnotPretrainOptimizer` as the optimizer in your train loop.\n",
    "\n",
    "Follow these steps to turn your train loop into enot \"pretrain\" loop:\n",
    "1. Pass `search_space.model_parameters()` to the optimizer instead of `search_space.parameters()`.\n",
    "1. Use `EnotPretrainOptimizer` as model optimizer.\n",
    "1. Wrap model step with a closure, and send the closure into `enot_optimizer.step(...)` method as parameter. This is necessary because `EnotPretrainOptimizer` does more than one step per batch. Alternatively, you can use `enot_optimizer.model_step(...)` in the combination with `enot_optimizer.step()` to make multiple gradient accumulations before the actual optimizer step.\n",
    "1. Call `search_space.sample_random_arch()` method on each validation step. This method samples a single architecture in the search space. This is necessary if you want measure the expectation of the search space performance, not the score of an individual random model.\n",
    "1. By default, `EnotPretrainOptimizer` require one batch of train data to initialize optimizations, so you should run `search_space.initialize_output_distribution_optimization(...)` before first model step. You can disable optimization checking in `EnotPretrainOptimizer` constructor (`check_recommended_optimizations` parameter), but it is not recomended.\n",
    "\n",
    "**IMPORTANT:**\n",
    "We set `N_EPOCHS`= 3 in this example to make tutorial execution faster. This is not enough for good pretrain quality, and you should set `N_EPOCHS`= 300 if you want to reproduce our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 3\n",
    "N_WARMUP_EPOCHS = 1\n",
    "\n",
    "train_loader = dataloaders['pretrain_train_dataloader']\n",
    "validation_loader = dataloaders['pretrain_validation_dataloader']\n",
    "\n",
    "# using `search_space.model_parameters()` as optimizable variables\n",
    "optimizer = SGD(params=search_space.model_parameters(), lr=0.06, momentum=0.9, weight_decay=1e-4)\n",
    "# using `EnotPretrainOptimizer` as a default optimizer\n",
    "enot_optimizer = EnotPretrainOptimizer(search_space=search_space, optimizer=optimizer)\n",
    "\n",
    "len_train_loader = len(train_loader)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=len_train_loader*N_EPOCHS, eta_min=1e-8)\n",
    "scheduler = WarmupScheduler(scheduler, warmup_steps=len_train_loader*N_WARMUP_EPOCHS)\n",
    "\n",
    "metric_function = accuracy\n",
    "loss_function = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    print(f'EPOCH #{epoch}')\n",
    "\n",
    "    search_space.train()\n",
    "    train_metrics_acc = {\n",
    "        'loss': 0.0,\n",
    "        'accuracy': 0.0,\n",
    "        'n': 0,\n",
    "    }\n",
    "    for inputs, labels in train_loader:\n",
    "        # By default, `EnotPretrainOptimizer` requires one batch of train data to initialize optimizations, \n",
    "        # so you should run `search_space.initialize_output_distribution_optimization(...)` before the first \n",
    "        # model step. You can disable optimization checking in `EnotPretrainOptimizer` constructor \n",
    "        # (`check_recommended_optimizations` parameter), but this is not recommended.\n",
    "        if not search_space.output_distribution_optimization_enabled:\n",
    "            search_space.initialize_output_distribution_optimization(inputs)\n",
    "\n",
    "        enot_optimizer.zero_grad()\n",
    "        # Wrapping model step and backward with closure.\n",
    "        # Alternatively, here is `enot_optimizer.model_step(...)` example usage for gradient accumulation:\n",
    "        #\n",
    "        # enot_optimizer.zero_grad()\n",
    "        # for inputs, labels in train_loader:\n",
    "        #\n",
    "        #     def closure():\n",
    "        #         ...\n",
    "        #\n",
    "        #     enot_optimizer.model_step(closure)\n",
    "        #     if (n + 1) % 10 == 0:\n",
    "        #         enot_optimizer.step()\n",
    "        #         enot_optimizer.zero_grad()\n",
    "        def closure():\n",
    "            pred_labels = search_space(inputs)\n",
    "            batch_loss = loss_function(pred_labels, labels)\n",
    "            batch_loss.backward()\n",
    "            batch_metric = metric_function(pred_labels, labels)\n",
    "\n",
    "            train_metrics_acc['loss'] += batch_loss.item()\n",
    "            train_metrics_acc['accuracy'] += batch_metric.item()\n",
    "            train_metrics_acc['n'] += 1\n",
    "\n",
    "        enot_optimizer.step(closure)\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    train_loss = train_metrics_acc['loss'] / train_metrics_acc['n']\n",
    "    train_accuracy = train_metrics_acc['accuracy'] / train_metrics_acc['n']\n",
    "\n",
    "    print('train metrics:')\n",
    "    print('  loss:', train_loss)\n",
    "    print('  accuracy:', train_accuracy)\n",
    "\n",
    "    search_space.eval()\n",
    "    validation_loss = 0\n",
    "    validation_accuracy = 0\n",
    "    for inputs, labels in validation_loader:\n",
    "\n",
    "        # Sample random architecture from the search space to estimate\n",
    "        # search space expected metrics.\n",
    "        search_space.sample_random_arch()\n",
    "\n",
    "        pred_labels = search_space(inputs)\n",
    "        batch_loss = loss_function(pred_labels, labels)\n",
    "        batch_metric = metric_function(pred_labels, labels)\n",
    "\n",
    "        validation_loss += batch_loss.item()\n",
    "        validation_accuracy += batch_metric.item()\n",
    "\n",
    "    n = len(validation_loader)\n",
    "    validation_loss /= n\n",
    "    validation_accuracy /= n\n",
    "\n",
    "    print('validation metrics:')\n",
    "    print('  loss:', validation_loss)\n",
    "    print('  accuracy:', validation_accuracy)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We pretrained search space for 3 epochs in this example. In this cell, we are downloading\n",
    "# search space checkpoint, pretrained for 300 epochs (for demonstration purposes).\n",
    "checkpoint_path = PROJECT_DIR / 'getting_started_pretrain_checkpoint.pth'\n",
    "download_getting_started_pretrain_checkpoint(checkpoint_path)\n",
    "\n",
    "search_space.load_state_dict(\n",
    "    torch.load(checkpoint_path)['model'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search best architecture\n",
    "Now, when you have a trained search space, you can run the \"search\" phase. The setup is similar to pretrain.\n",
    "\n",
    "Follow these steps to turn your train loop into enot \"search\" loop:\n",
    "Pass `search_space.architecture_parameters()` to the optimizer instead of `search_space.parameters()`.\n",
    "1. Use `EnotSearchOptimizer` as model optimizer.\n",
    "1. To initialize latency of all operations, your should use `initialize_latency` function from `enot.latency`. Currently, we support `mmac` latency type and two third-party calculators for this latency type: `mmac.thop`, `mmac.pthflops`.\n",
    "1. Wrap model step with a closure, and send the closure into `enot_optimizer.step(...)` method as parameter. This is necessary because `EnotSearchOptimizer` does more than one step per batch. Alternatively, you can use `enot_optimizer.model_step(...)` in the combination with `enot_optimizer.step()` to make multiple gradient accumulations before the actual optimizer step.\n",
    "1. To take into consideration latency during search, you should sum latency loss with your main loss.\n",
    "1. You should run validation on the best architecture. The best architecture is an architecture constructed from operations with highest probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "latency_loss_weight = 2e-3\n",
    "\n",
    "# using `search_space.architecture_parameters()` as optimizable variables\n",
    "optimizer = RAdam(search_space.architecture_parameters(), lr=0.01)\n",
    "# using `EnotPretrainOptimizer` as a default optimizer\n",
    "\n",
    "enot_optimizer = EnotSearchOptimizer(search_space, optimizer)\n",
    "\n",
    "metric_function = accuracy\n",
    "loss_function = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "train_loader = dataloaders['search_train_dataloader']\n",
    "validation_loader = dataloaders['search_validation_dataloader']\n",
    "\n",
    "latency_type = 'mmac' # or mmac.thop, mmac.pthflops\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    print(f'EPOCH #{epoch}')\n",
    "\n",
    "    search_space.train()\n",
    "    train_metrics_acc = {\n",
    "        'loss': 0.0,\n",
    "        'accuracy': 0.0,\n",
    "        'n': 0,\n",
    "    }\n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        # Initialize latency.\n",
    "        if latency_type and search_space.latency_type is None:\n",
    "            latency_container = initialize_latency(latency_type, search_space, (inputs, ))\n",
    "            print(f'Constant latency = {latency_container.constant_latency}')\n",
    "            print(f'Min, mean and max latencies of search space: '\n",
    "                  f'{min_latency(latency_container)}, '\n",
    "                  f'{mean_latency(latency_container)}, '\n",
    "                  f'{max_latency(latency_container)}')\n",
    "\n",
    "        enot_optimizer.zero_grad()\n",
    "\n",
    "        # Wrapping model step and backward with closure.\n",
    "        def closure():\n",
    "            pred_labels = search_space(inputs)\n",
    "            batch_loss = loss_function(pred_labels, labels)\n",
    "\n",
    "            # adding latency loss to main loss\n",
    "            if latency_loss_weight is not None and latency_loss_weight != 0:\n",
    "                batch_loss += search_space.loss_latency_expectation * latency_loss_weight\n",
    "\n",
    "            batch_loss.backward()\n",
    "            batch_metric = metric_function(pred_labels, labels)\n",
    "\n",
    "            train_metrics_acc['loss'] += batch_loss.item()\n",
    "            train_metrics_acc['accuracy'] += batch_metric.item()\n",
    "            train_metrics_acc['n'] += 1\n",
    "\n",
    "        enot_optimizer.step(closure)\n",
    "\n",
    "    train_loss = train_metrics_acc['loss'] / train_metrics_acc['n']\n",
    "    train_accuracy = train_metrics_acc['accuracy'] / train_metrics_acc['n']\n",
    "    arch_probabilities = np.array(search_space.architecture_probabilities)\n",
    "\n",
    "    print('train metrics:')\n",
    "    print('  loss:', train_loss)\n",
    "    print('  accuracy:', train_accuracy)\n",
    "    print('  arch_probabilities:')\n",
    "    print(arch_probabilities)\n",
    "\n",
    "    search_space.eval()\n",
    "\n",
    "    # selecting best architecture for validation\n",
    "    search_space.sample_best_arch()\n",
    "\n",
    "    validation_loss = 0\n",
    "    validation_accuracy = 0\n",
    "    for inputs, labels in validation_loader:\n",
    "        pred_labels = search_space(inputs)\n",
    "        batch_loss = loss_function(pred_labels, labels)\n",
    "        batch_metric = metric_function(pred_labels, labels)\n",
    "\n",
    "        validation_loss += batch_loss.item()\n",
    "        validation_accuracy += batch_metric.item()\n",
    "\n",
    "    n = len(validation_loader)\n",
    "    validation_loss /= n\n",
    "    validation_accuracy /= n\n",
    "\n",
    "    print('validation metrics:')\n",
    "    print('  loss:', validation_loss)\n",
    "    print('  accuracy:', validation_accuracy)\n",
    "    if search_space.latency_type is not None:\n",
    "        # getting latency of the best architecture\n",
    "        latency = best_arch_latency(search_space)\n",
    "        print('  latency:', latency)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune model with the best architecture\n",
    "Now we take our best architecture from search space, and create a regular model using it. Then we run finetune procedure (usual training loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get regular model with the best architecture\n",
    "best_model = search_space.get_network_with_best_arch().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "optimizer = SGD(best_model.parameters(), lr=2e-4)\n",
    "scheduler = None\n",
    "metric_function = accuracy\n",
    "loss_function = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "train_loader = dataloaders['tune_train_dataloader']\n",
    "validation_loader = dataloaders['tune_validation_dataloader']\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    print(f'EPOCH #{epoch}')\n",
    "\n",
    "    best_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        pred_labels = best_model(inputs)\n",
    "        batch_loss = loss_function(pred_labels, labels)\n",
    "        batch_loss.backward()\n",
    "        batch_metric = metric_function(pred_labels, labels)\n",
    "            \n",
    "        train_loss += batch_loss.item()\n",
    "        train_accuracy += batch_metric.item()\n",
    "\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    n = len(train_loader)\n",
    "    train_loss /= n\n",
    "    train_accuracy /= n\n",
    "\n",
    "    print('train metrics:')\n",
    "    print('  loss:', train_loss)\n",
    "    print('  accuracy:', train_accuracy)\n",
    "    \n",
    "    best_model.eval()    \n",
    "    validation_loss = 0\n",
    "    validation_accuracy = 0\n",
    "    for inputs, labels in validation_loader:\n",
    "        pred_labels = best_model(inputs)\n",
    "        batch_loss = loss_function(pred_labels, labels)\n",
    "        batch_metric = metric_function(pred_labels, labels)\n",
    "\n",
    "        validation_loss += batch_loss.item()\n",
    "        validation_accuracy += batch_metric.item()\n",
    "    \n",
    "    n = len(validation_loader)\n",
    "    validation_loss /= n\n",
    "    validation_accuracy /= n\n",
    "    \n",
    "    print('validation metrics:')\n",
    "    print('  loss:', validation_loss)\n",
    "    print('  accuracy:', validation_accuracy)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
