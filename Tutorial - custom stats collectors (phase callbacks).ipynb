{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom stats collector (phase callbacks)\n",
    "This notebook describes how you can implement your own stats collector.\n",
    "\n",
    "### Notebook consists of next main stages:\n",
    "1. Setup the environment\n",
    "1. Prepare dataset and create dataloaders\n",
    "1. Create the model and move it to search space\n",
    "1. Define class for custom stats collector\n",
    "1. Run Pretrain, Search, Tune phases with custom stats collector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup the environment\n",
    "First, let's set up the environment and common imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "# You should change to free GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch_optimizer import RAdam\n",
    "\n",
    "from enot.models import SearchSpaceModel\n",
    "from enot.models.mobilenet import build_mobilenet\n",
    "from enot.phases import pretrain\n",
    "from enot.phases import search\n",
    "from enot.phases import train\n",
    "from enot.utils.data.csv_annotation_dataset import CsvAnnotationDataset\n",
    "from enot.utils.data.dataloaders import create_data_loader\n",
    "from enot.utils.data.dataloaders import create_data_loader_from_csv_annotation\n",
    "from enot.utils.stats_collector import StatsCollector\n",
    "\n",
    "from enot_utils.metric_utils import accuracy\n",
    "from enot_utils.schedulers import WarmupScheduler\n",
    "\n",
    "from tutorial_utils.checkpoints import download_getting_started_pretrain_checkpoint\n",
    "from tutorial_utils.dataset import create_imagenette_annotation\n",
    "from tutorial_utils.dataset import download_imagenette\n",
    "from tutorial_utils.dataset import create_imagenette_train_transform\n",
    "from tutorial_utils.dataset import create_imagenette_validation_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the next cell we setup all required dirs\n",
    "\n",
    "* `ENOT_HOME_DIR` - is root dir for all other dirs\n",
    "* `ENOT_DATASETS_DIR` - is root dir for datasets (imagenette2)\n",
    "* `PROJECT_DIR` - is root dir for output data (checkpoints, logs...) of current tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENOT_HOME_DIR = Path.home() / '.enot'\n",
    "ENOT_DATASETS_DIR = ENOT_HOME_DIR / 'datasets'\n",
    "PROJECT_DIR = ENOT_HOME_DIR / 'stats_collector'\n",
    "\n",
    "ENOT_HOME_DIR.mkdir(exist_ok=True)\n",
    "ENOT_DATASETS_DIR.mkdir(exist_ok=True)\n",
    "PROJECT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset and create dataloaders\n",
    "For this tutorial we add `limit` parameter for default `Dataset`. `limit` defines max number of output images of `Dataset`. Small datasets allow to create phases (pretrain, search, tune) with short and clear log. Pretrain, search and tune dataloaders contains 2 batch with 2 samples in each batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CsvAnnotationDatasetWithLimit(CsvAnnotationDataset):\n",
    "    def __init__(self, csv_annotation_path, root_dir=None, transform=None, limit=None):\n",
    "        self._limit = limit\n",
    "        super().__init__(csv_annotation_path, root_dir=root_dir, transform=transform)\n",
    "    \n",
    "    def __len__(self):\n",
    "        n = super().__len__()\n",
    "        if self._limit is None:\n",
    "            return n\n",
    "        \n",
    "        return min(self._limit, n)\n",
    "\n",
    "\n",
    "input_size = (224, 224)\n",
    "batch_size = 2\n",
    "dataset_dir = download_imagenette(\n",
    "    dataset_root_dir=ENOT_DATASETS_DIR, imagenette_kind='imagenette2-320',\n",
    ")\n",
    "annotations = create_imagenette_annotation(\n",
    "    dataset_dir=dataset_dir, project_dir=PROJECT_DIR, random_seed=42,\n",
    ")\n",
    "train_transform = create_imagenette_train_transform(input_size)\n",
    "validation_transform = create_imagenette_validation_transform(input_size)\n",
    "\n",
    "pretrain_and_tune_train_dataloader = create_data_loader(\n",
    "    dataset=CsvAnnotationDatasetWithLimit(\n",
    "        annotations['train'], \n",
    "        transform=train_transform,\n",
    "        limit=2*batch_size,\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "pretrain_and_tune_validation_dataloader = create_data_loader(\n",
    "    dataset=CsvAnnotationDatasetWithLimit(\n",
    "        annotations['train'], \n",
    "        transform=validation_transform,\n",
    "        limit=2*batch_size,\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "search_train_dataloader = create_data_loader(\n",
    "    dataset=CsvAnnotationDatasetWithLimit(\n",
    "        annotations['search'],\n",
    "        transform=train_transform,\n",
    "        limit=2*batch_size,\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "search_validation_dataloader = create_data_loader(\n",
    "    dataset=CsvAnnotationDatasetWithLimit(\n",
    "        annotations['search'],\n",
    "        transform=validation_transform,\n",
    "        limit=2*batch_size,\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the model and move it to search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The search space will have these ops to choose from in each layer\n",
    "# Short format for operations is 'Name_param1=value1_param2=value2...'\n",
    "# MIB is MNv2 inverted bottleneck. k is kernel size, t is expansion ratio\n",
    "# See more in-depth info in \"Tutorial - adding custom operations\"\n",
    "SEARCH_OPS = [\n",
    "    'MIB_k=3_t=6',\n",
    "    'MIB_k=5_t=6',\n",
    "    'MIB_k=7_t=6',\n",
    "]\n",
    "\n",
    "# build model\n",
    "model = build_mobilenet(\n",
    "    search_ops=SEARCH_OPS,\n",
    "    num_classes=10,\n",
    "    blocks_out_channels=[24, 32, 64, 96, 160, 320],\n",
    "    blocks_count=[2, 2, 2, 1, 2, 1],\n",
    "    blocks_stride=[2, 2, 2, 1, 2, 1],\n",
    ")\n",
    "# move model to search space\n",
    "search_space = SearchSpaceModel(model).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define class for custom stats collector\n",
    "All custom stats collectors subclass the `StatsCollector`. We define new custom stats collector, which print name and arguments of each callback on every call. To turn on `on_train_batch_result` and `on_validation_batch_result` callbacks we set `need_train_batch_result` and `need_validation_batch_result` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStatsCollector(StatsCollector):\n",
    "    @property\n",
    "    def need_train_batch_result(self):\n",
    "        \"\"\"\n",
    "        If this property is True then on_train_batch_result will be called in phase loop, \n",
    "        otherwise on_train_batch_result will be ignored.\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def need_validation_batch_result(self):\n",
    "        \"\"\"\n",
    "        If this property is True then on_train_batch_result will be called in phase loop, \n",
    "        otherwise on_train_batch_result will be ignored.\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    def on_phase_start(self, phase_name):\n",
    "        print(f'on_phase_start: {phase_name}')\n",
    "\n",
    "    def on_phase_end(self, phase_name):\n",
    "        print(f'on_phase_end: {phase_name}')\n",
    "\n",
    "    def on_epoch_start(self, epoch):\n",
    "        print(f'on_epoch_start: #{epoch}')\n",
    "\n",
    "    def on_epoch_end(self, epoch, stats):\n",
    "        print(f'on_epoch_end: #{epoch}\\n{stats}')\n",
    "\n",
    "    def on_train_start(self):\n",
    "        print('on_train_start')\n",
    "\n",
    "    def on_train_end(self, stats):\n",
    "        print(f'on_train_end:\\n{stats}')\n",
    "\n",
    "    def on_validation_start(self):\n",
    "        print('on_validation_start')\n",
    "\n",
    "    def on_validation_end(self, stats):\n",
    "        print(f'on_validation_end:\\n{stats}')\n",
    "\n",
    "    def on_train_batch_start(self, batch):\n",
    "        print(f'on_train_batch_start: #{batch}')\n",
    "\n",
    "    def on_train_batch_result(self, batch, predicted, original, process_index, sample_index):\n",
    "        print(f'on_train_batch_result: #{batch} (process_index = {process_index}, sample_index = {sample_index})')\n",
    "\n",
    "    def on_train_batch_end(self, batch, stats):\n",
    "        print(f'on_train_batch_end: #{batch}\\n{stats}')\n",
    "\n",
    "    def on_validation_batch_start(self, batch):\n",
    "        print(f'on_validation_batch_start: #{batch}')\n",
    "\n",
    "    def on_validation_batch_result(self, batch, predicted, original, process_index, sample_index):\n",
    "        print(f'on_validation_batch_result: #{batch} (process_index = {process_index}, sample_index = {sample_index})')\n",
    "\n",
    "    def on_validation_batch_end(self, batch, stats):\n",
    "        print(f'on_validation_batch_end: #{batch}\\n{stats}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Pretrain, Search, Tune phases with custom stats collector\n",
    "We run all phases, with our custom stats collector.\n",
    "\n",
    "**IMPORTANT:**<br>\n",
    "Default logging and tensorboard logging stats collectors will be replaced by user defined stats collector. If you need functional of default stats collectors, then you should to add them manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define directory for text logs and tensorboard logs\n",
    "pretrain_dir = PROJECT_DIR / 'pretrain'\n",
    "pretrain_dir.mkdir(exist_ok=True)\n",
    "\n",
    "N_EPOCHS = 1\n",
    "\n",
    "optimizer = SGD(params=search_space.model_parameters(), lr=0.06, momentum=0.9, weight_decay=1e-4)\n",
    "loss_function = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "pretrain(\n",
    "    search_space=search_space,\n",
    "    exp_dir=pretrain_dir,\n",
    "    train_loader=pretrain_and_tune_train_dataloader,\n",
    "    valid_loader=pretrain_and_tune_validation_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    metric_function=accuracy,\n",
    "    loss_function=loss_function,\n",
    "    epochs=N_EPOCHS,\n",
    "    stats_collectors=[MyStatsCollector()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define directory for text logs and tensorboard logs\n",
    "search_dir = PROJECT_DIR / 'search'\n",
    "search_dir.mkdir(exist_ok=True)\n",
    "\n",
    "optimizer = RAdam(search_space.architecture_parameters(), lr=0.01)\n",
    "\n",
    "search(\n",
    "    search_space=search_space,\n",
    "    exp_dir=search_dir,\n",
    "    search_loader=search_train_dataloader,\n",
    "    valid_loader=search_validation_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    loss_function=loss_function,\n",
    "    metric_function=accuracy,\n",
    "    latency_loss_weight=2.0e-3,\n",
    "    epochs=5,\n",
    "    stats_collectors=[MyStatsCollector()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get regular model with best architecture\n",
    "best_model = search_space.get_network_with_best_arch().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define directory for text logs and tensorboard logs\n",
    "tune_dir = PROJECT_DIR / 'tune'\n",
    "tune_dir.mkdir(exist_ok=True)\n",
    "\n",
    "optimizer = RAdam(best_model.parameters(), lr=5e-3, weight_decay=4e-5)\n",
    "\n",
    "train(\n",
    "    model=best_model,\n",
    "    exp_dir=tune_dir,\n",
    "    train_loader=pretrain_and_tune_train_dataloader,\n",
    "    valid_loader=pretrain_and_tune_validation_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    loss_function=loss_function,\n",
    "    metric_function=accuracy,\n",
    "    epochs=5,\n",
    "    stats_collectors=[MyStatsCollector()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
