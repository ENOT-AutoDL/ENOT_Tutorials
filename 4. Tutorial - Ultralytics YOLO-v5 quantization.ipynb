{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fda310e",
   "metadata": {},
   "source": [
    "## Automatic quantization and optimized inference for YOLO-v5 with enot-lite backend\n",
    "\n",
    "This notebook demonstrates simple procedure for Ultralytics Yolo-v5 quantization.\n",
    "\n",
    "Our quantization process consists of quantized model calibration, quantization thresholds adjustment and weight fine-tuning using distillation. Finally, we demonstrate inference of our quantized model using [enot-lite](https://enot-lite.rtd.enot.ai/en/stable/) framework.\n",
    "\n",
    "### Main chapters of this notebook:\n",
    "1. Setup the environment\n",
    "1. Prepare dataset and create dataloaders\n",
    "1. Baseline Yolo-v5 onnx creation\n",
    "1. Quantize Yolo-v5\n",
    "1. Measure speed of default YOLO inferenced via default pytorch and quantized YOLO inferenced via enot-lite with TensorRT int8 backend.\n",
    "1. Measure mAP for float and quantized versions\n",
    "\n",
    "Before running this example make sure that TensorRT supports your GPU for int8 inference  (``cuda compute capability`` > 6.1, as described [here](https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html#hardware-precision-matrix))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2d3e9b",
   "metadata": {},
   "source": [
    "## Setup the environment\n",
    "\n",
    "First, let's set up the environment and make some common imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7967df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae062b0",
   "metadata": {},
   "source": [
    "1. Install enot-autodl and enot-lite libraries and create jupyter kernel with them.\n",
    "2. Clone specific commit from YOLOv5 repository: https://github.com/ultralytics/yolov5/commit/f76a78e7078185ecdc67470d8658103cf2067c81\n",
    "3. Replace the val.py script with our val.py\n",
    "4. Replace path to COCO dataset folder in 'yolov5/data/coco.yaml' file. If you do not have pre-downloaded MS COCO dataset - you can leave it as is and the dataset will be automatically downloaded.\n",
    "\n",
    "Steps 2 and 3 will be done with these commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d02fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/ultralytics/yolov5\n",
    "! cd yolov5/ && git checkout f76a78e7078185ecdc67470d8658103cf2067c81\n",
    "! cp tutorial_utils/val.py yolov5/val.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d93b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('yolov5/')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "# quantization procedure\n",
    "from enot.quantization import TrtFakeQuantizedModel\n",
    "from enot.quantization import DefaultQuantizationDistiller\n",
    "\n",
    "# optimized inference\n",
    "from enot_lite.benchmark import Benchmark\n",
    "from enot_lite.type import BackendType\n",
    "from enot_lite.type import ModelType\n",
    "\n",
    "# converters from onnx to pytorch\n",
    "from onnx2torch import convert\n",
    "from onnx2torch.utils.custom_export_to_onnx import OnnxToTorchModuleWithCustomExport\n",
    "\n",
    "# dataset creation functions\n",
    "from yolov5.utils.dataloaders import create_dataloader\n",
    "from yolov5.utils.general import check_dataset\n",
    "\n",
    "# onnx conversion function\n",
    "from yolov5.export import export_onnx\n",
    "\n",
    "# common validation function for Ultralytics YOLO models\n",
    "from yolov5.val import run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad45e20",
   "metadata": {},
   "source": [
    "### In the following cell we setup all necessary contants\n",
    "\n",
    "* `ENOT_HOME_DIR` - ENOT framework home directory\n",
    "* `PROJECT_DIR` - project directory to save training logs, checkpoints, ...\n",
    "* `ONNX_MODEL_PATH` - onnx model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb6f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENOT_HOME_DIR = Path.home() / '.enot'\n",
    "ENOT_DATASETS_DIR = ENOT_HOME_DIR / 'datasets/coco_for_yolo'\n",
    "PROJECT_DIR = ENOT_HOME_DIR / 'enot-lite_quantization'\n",
    "QUANT_ONNX_PATH = PROJECT_DIR / 'yolov5s_trt_int8.onnx'\n",
    "ONNX_PATH = PROJECT_DIR / 'yolov5s_fp32.onnx'\n",
    "\n",
    "ENOT_HOME_DIR.mkdir(exist_ok=True)\n",
    "PROJECT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "IMG_SIZE = 640\n",
    "IMG_SHAPE = (BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f2ec45",
   "metadata": {},
   "source": [
    "## Prepare dataset and create dataloaders\n",
    "\n",
    "We will use MS COCO dataset in this example.\n",
    "\n",
    "\n",
    "`create_dataloader` and `check_dataset` functions prepare datasets for you in this example; specifically, it:\n",
    "1. downloads and unpacks dataset into folder pointed out in `yolov5/data/coco.yaml`;\n",
    "1. creates and returns train and validation dataloaders.\n",
    "\n",
    "**IMPORTANT NOTE**: since this is example notebook we will train and validate model in **THE SAME DATASET**. For better performance and generalization use separate dataset for train and val procedure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4073b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7cc37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('yolov5/data/coco.yaml', 'r') as f:\n",
    "    coco_cfg = yaml.load(f, yaml.Loader)\n",
    "\n",
    "coco_cfg['path'] = ENOT_DATASETS_DIR.as_posix()\n",
    "\n",
    "with open('yolov5/data/coco.yaml', 'w') as f:\n",
    "    yaml.dump(coco_cfg, f)\n",
    "\n",
    "data = check_dataset('yolov5/data/coco.yaml', autodownload=False)\n",
    "\n",
    "valid_dataloader = create_dataloader(data[\"val\"], IMG_SIZE, BATCH_SIZE, 32, False, pad=0.5, rect=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e874892",
   "metadata": {},
   "source": [
    "## Baseline YOLO-v5 onnx creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba172c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the default YOLOv5 model contains conditional execution ('if' nodes), we have to save\n",
    "# it to ONNX format and convert back to PyTorch to perform quantization.\n",
    "\n",
    "\n",
    "yolov5s = torch.hub.load(\n",
    "    'ultralytics/yolov5', \n",
    "    'yolov5s', \n",
    "    autoshape=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5837bfc8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# You can also get ONNX YOLOv5 model by using Utlralytic's default export script:\n",
    "# `python export.py --weights yolov5.pt --include onnx --dynamic`\n",
    "# We simply export our model to ONNX with the function defined in this cell.\n",
    "\n",
    "\n",
    "export_onnx(\n",
    "    yolov5s, \n",
    "    torch.zeros(*IMG_SHAPE, dtype=torch.float32), \n",
    "    Path(ONNX_PATH), \n",
    "    opset=13, \n",
    "    train=False, \n",
    "    dynamic=True, \n",
    "    simplify=True,\n",
    ")\n",
    "\n",
    "regular_model = convert(ONNX_PATH).cuda()\n",
    "regular_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d4eea0",
   "metadata": {},
   "source": [
    "## Quantization YOLO-v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be645c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define function for converting dataset samples to model inputs.\n",
    "# This is required since we have to pass samples into any network in an unified manner.\n",
    "# This function may also perform different manipulations with images as done below.\n",
    "# For complete documentation of such conversion functions, see \n",
    "# https://enot-autodl.rtd.enot.ai/en/latest/reference_documentation/dataloader2model.html\n",
    "\n",
    "\n",
    "def sample_to_model_inputs(x):\n",
    "    # x[0] is the first item from dataloader sample. Sample is a tuple where 0'th element is a tensor with images.\n",
    "    x = x[0]\n",
    "    \n",
    "    # Model is on CUDA, so input images should also be on CUDA.\n",
    "    x = x.cuda()  \n",
    "    \n",
    "    # Converting tensor from int8 to float data type.\n",
    "    x = x.float()\n",
    "    \n",
    "    # YOLOv5 image normalization (0-255 to 0-1 normalization)\n",
    "    x /= 255  \n",
    "    return (x, ), {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_quantized_model = TrtFakeQuantizedModel(regular_model).cuda()\n",
    "\n",
    "# Distill model quantization thresholds and weights using RMSE loss.\n",
    "# Note that we are using **valid_dataloader** for fast calculation. \n",
    "# For real purpose you have to use your train data, at least some part of it.\n",
    "\n",
    "dist = DefaultQuantizationDistiller(\n",
    "    quantized_model=fake_quantized_model,\n",
    "    dataloader=valid_dataloader,\n",
    "    sample_to_model_inputs=sample_to_model_inputs,\n",
    "    device='cuda',\n",
    "    logdir=PROJECT_DIR,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "dist.distill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008bb58c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fake_quantized_model.cuda()\n",
    "fake_quantized_model.enable_quantization_mode(True)\n",
    "fake_quantized_model.cpu()\n",
    "\n",
    "torch.onnx.export(\n",
    "    model=fake_quantized_model,\n",
    "    args=torch.ones(*IMG_SHAPE),\n",
    "    f=QUANT_ONNX_PATH,\n",
    "    input_names=['images'],\n",
    "    output_names=['output'],\n",
    "    opset_version=13,\n",
    "    dynamic_axes={\n",
    "        'images': {0 : 'batch_size'}\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2382f25f",
   "metadata": {},
   "source": [
    "## Speed measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f48f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19d9e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "torch_input = torch.ones((BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE), dtype=torch.float32).cpu()\n",
    "onnx_input = {\n",
    "    'images': np.ones((BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE), dtype=np.float32)\n",
    "}\n",
    "\n",
    "benchmark = Benchmark(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    torch_model=yolov5s,\n",
    "    torch_input=torch_input,\n",
    "    backends=[\n",
    "        BackendType.TORCH_CUDA, \n",
    "        (BackendType.AUTO_GPU, ModelType.YOLO_V5),\n",
    "    ],\n",
    "    onnx_model=QUANT_ONNX_PATH,\n",
    "    onnx_input=onnx_input,\n",
    ")\n",
    "benchmark.run()\n",
    "benchmark.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d75d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e946a32e",
   "metadata": {},
   "source": [
    "## mAP evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d325666",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {\n",
    "    'data':'yolov5/data/coco.yaml',\n",
    "    'weights':'yolov5s.pt',\n",
    "    'half': True,\n",
    "    'batch_size': 8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf53f3e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run(**opt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6318167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82038aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt['use_enot'] = True\n",
    "opt['enot_weights'] = QUANT_ONNX_PATH\n",
    "opt['half'] = False\n",
    "opt['device'] = 'cpu'\n",
    "opt['batch_size'] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff882f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(**opt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa82bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
