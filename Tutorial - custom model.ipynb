{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom modelÂ \n",
    "In this notebook we describe how you can implement your own model. The model which can not be built by \"block models builders\" from framework.\n",
    "\n",
    "### Notebook consists of next main stages:\n",
    "1. Setup the environment\n",
    "1. Define, create the model and move it to search space\n",
    "1. Prepare dataset and create dataloaders\n",
    "1. Check pretrain, search and tune phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup the environment\n",
    "First, let's set up the environment and common imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy your license file to $HOME/.enot/enot.lic or set full path to licence file \n",
    "# through environment variable ENOT_LIC_FILE\n",
    "#\n",
    "# Important note: no quotes\n",
    "# %env ENOT_LIC_FILE=/FULL/PATH/TO/your_company.lic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "# You should change to free GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch_optimizer import RAdam\n",
    "\n",
    "from enot.models import SearchSpaceModel\n",
    "from enot.models.mobilenet import MobileNetBaseHead\n",
    "from enot.models.mobilenet import MobileNetBaseStem\n",
    "from enot.models.operations import SearchableMobileInvertedBottleneck\n",
    "from enot.models.operations import SearchableFuseableSkipConv\n",
    "from enot.models.operations import SearchVariantsContainer\n",
    "from enot.phases import pretrain\n",
    "from enot.phases import search\n",
    "from enot.phases import train\n",
    "\n",
    "from enot_utils.metric_utils import accuracy\n",
    "from enot_utils.schedulers import WarmupScheduler\n",
    "\n",
    "from tutorial_utils.checkpoints import download_getting_started_pretrain_checkpoint\n",
    "from tutorial_utils.dataset import create_imagenette_dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the next cell we setup all required dirs\n",
    "\n",
    "* `ENOT_HOME_DIR` - is root dir for all other dirs\n",
    "* `ENOT_DATASETS_DIR` - is root dir for datasets (imagenette2)\n",
    "* `PROJECT_DIR` - is root dir for output data (checkpoints, logs...) of current tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENOT_HOME_DIR = Path.home() / '.enot'\n",
    "ENOT_DATASETS_DIR = ENOT_HOME_DIR / 'datasets'\n",
    "PROJECT_DIR = ENOT_HOME_DIR / 'custom_model'\n",
    "\n",
    "ENOT_HOME_DIR.mkdir(exist_ok=True)\n",
    "ENOT_DATASETS_DIR.mkdir(exist_ok=True)\n",
    "PROJECT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare dataset and create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = create_imagenette_dataloaders(\n",
    "    dataset_root_dir=ENOT_DATASETS_DIR, \n",
    "    project_dir=PROJECT_DIR,\n",
    "    input_size=(224, 224),\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the model and move it to search space\n",
    "To create your custom model you should build it as common pytorch model with the following rules:\n",
    "1. Search variants must be placed in `SearchVariantsContainer` module.\n",
    "2. All `SearchVariantsContainer` must contain the same number of operations. Operations of different containers can be different, but all containers must contain the same number of operations. **This restriction will be removed in the future versions.**\n",
    "3. Every operation in SearchVariantsContainer should be created as in \"Tutorial - adding custom ops\" or you can use predefined operations from framework (`enot.model.operations`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stem = MobileNetBaseStem(\n",
    "            in_channels=3\n",
    "        )\n",
    "        self.body = nn.ModuleList([\n",
    "            # 3 blocks with 3 search variants in every block \n",
    "            self.build_search_variants_1(16, 24, 2),\n",
    "            self.build_search_variants_1(24, 24, 1),\n",
    "            self.build_search_variants_1(24, 24, 1),\n",
    "            # 2 fixed blocks\n",
    "            self.build_mib_k3_e6(24, 32, 2),\n",
    "            self.build_mib_k3_e6(32, 32, 1),\n",
    "            # 3 blocks with 3 search variants in every block\n",
    "            self.build_search_variants_1(32, 64, 2),\n",
    "            self.build_search_variants_1(64, 64, 1),\n",
    "            self.build_search_variants_1(64, 64, 1),\n",
    "            # 1 fixed block\n",
    "            self.build_mib_k3_e6(64, 96, 1),\n",
    "            # 2 blocks with 3 search variants in every block\n",
    "            self.build_search_variants_2(96, 160, 2),\n",
    "            self.build_search_variants_2(160, 160, 1),\n",
    "            # 1 block with 3 search variants\n",
    "            self.build_search_variants_2(160, 320, 1),\n",
    "        ])\n",
    "        self.head = MobileNetBaseHead(\n",
    "            bottleneck_channels=320,\n",
    "            last_channels=1280, \n",
    "            num_classes=10,\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_search_variants_1(in_channels, out_channels, stride):\n",
    "        return SearchVariantsContainer([\n",
    "            SearchableMobileInvertedBottleneck(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=stride,\n",
    "                expand_ratio=6,\n",
    "            ),\n",
    "            SearchableMobileInvertedBottleneck(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=5,\n",
    "                stride=stride,\n",
    "                expand_ratio=6,\n",
    "            ),\n",
    "            SearchableMobileInvertedBottleneck(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=7,\n",
    "                stride=stride,\n",
    "                expand_ratio=3,\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_search_variants_2(in_channels, out_channels, stride):\n",
    "        return SearchVariantsContainer([\n",
    "            SearchableMobileInvertedBottleneck(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=stride,\n",
    "                expand_ratio=6,\n",
    "            ),\n",
    "            SearchableMobileInvertedBottleneck(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=5,\n",
    "                stride=stride,\n",
    "                expand_ratio=6,\n",
    "            ),\n",
    "            SearchableFuseableSkipConv(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                stride=stride,\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_mib_k3_e6(in_channels, out_channels, stride):\n",
    "        return SearchableMobileInvertedBottleneck(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=stride,\n",
    "                expand_ratio=6,\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        \n",
    "        for block in self.body:\n",
    "            x = block(x)\n",
    "            \n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = MyModel()\n",
    "# move model to search space\n",
    "search_space = SearchSpaceModel(model, train_loader=dataloaders['pretrain_train_dataloader']).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check pretrain, search and tune phases\n",
    "\n",
    "**IMPORTANT:**<br>\n",
    "`N_EPOCHS` of pretrain should be in range >= 100, if you wanna get good pretrain. In this tutorial we set `N_EPOCHS` = 3 and just check phases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define directory for text logs and tensorboard logs\n",
    "pretrain_dir = PROJECT_DIR / 'pretrain'\n",
    "pretrain_dir.mkdir(exist_ok=True)\n",
    "\n",
    "N_EPOCHS = 3\n",
    "N_WARMUP_EPOCHS = 1\n",
    "len_train = len(dataloaders['pretrain_train_dataloader'])\n",
    "\n",
    "optimizer = SGD(params=search_space.model_parameters(), lr=0.06, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=len_train*N_EPOCHS, eta_min=1e-8)\n",
    "scheduler = WarmupScheduler(scheduler, warmup_steps=len_train*N_WARMUP_EPOCHS)\n",
    "loss_function = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "pretrain(\n",
    "    search_space=search_space,\n",
    "    exp_dir=pretrain_dir,\n",
    "    train_loader=dataloaders['pretrain_train_dataloader'],\n",
    "    valid_loader=dataloaders['pretrain_validation_dataloader'],\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    metric_function=accuracy,\n",
    "    loss_function=loss_function,\n",
    "    epochs=N_EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define directory for text logs and tensorboard logs\n",
    "search_dir = PROJECT_DIR / 'search'\n",
    "search_dir.mkdir(exist_ok=True)\n",
    "\n",
    "optimizer = RAdam(search_space.architecture_parameters(), lr=0.01)\n",
    "\n",
    "search(\n",
    "    search_space=search_space,\n",
    "    exp_dir=search_dir,\n",
    "    search_loader=dataloaders['search_train_dataloader'],\n",
    "    valid_loader=dataloaders['search_validation_dataloader'],\n",
    "    optimizer=optimizer,\n",
    "    loss_function=loss_function,\n",
    "    metric_function=accuracy,\n",
    "    latency_loss_weight=2.0e-3,\n",
    "    epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get regular model with best architecture\n",
    "best_model = search_space.get_network_with_best_arch().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define directory for text logs and tensorboard logs\n",
    "tune_dir = PROJECT_DIR / 'tune'\n",
    "tune_dir.mkdir(exist_ok=True)\n",
    "\n",
    "optimizer = RAdam(best_model.parameters(), lr=5e-3, weight_decay=4e-5)\n",
    "\n",
    "train(\n",
    "    model=best_model,\n",
    "    exp_dir=tune_dir,\n",
    "    train_loader=dataloaders['tune_train_dataloader'],\n",
    "    valid_loader=dataloaders['tune_validation_dataloader'],\n",
    "    optimizer=optimizer,\n",
    "    loss_function=loss_function,\n",
    "    metric_function=accuracy,\n",
    "    epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enot_venv",
   "language": "python",
   "name": ".enot_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
