{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic quantization for enot-lite\n",
    "\n",
    "This notebook demonstrates simple end2end pipeline for MobileNetV2 quantization.\n",
    "\n",
    "Our quantization process consists of quantized model calibration, quantization threshold adjustment and weight fine-tuning using distillation. Finally, we demonstrate inference of our quantized model using [enot-lite](https://enot-lite.rtd.enot.ai/en/latest/) framework.\n",
    "\n",
    "### Main chapters of this notebook:\n",
    "1. Setup the environment\n",
    "1. Prepare dataset and create dataloaders\n",
    "1. Evaluate pretrained MobileNetV2 from torchvision\n",
    "1. End2end quantization with ENOT\n",
    "1. Inference using enot-lite with TensorRT int8 backend\n",
    "\n",
    "Before running this example make sure that TensorRT supports your GPU for int8 inference  (``cuda compute capability`` > 6.1, as described [here](https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html#hardware-precision-matrix))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the environment\n",
    "\n",
    "First, let's set up the environment and make some common imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "# You may need to change this variable to match free GPU index\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common:\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from tutorial_utils.dataset import create_imagenet10k_dataloaders\n",
    "from tutorial_utils.train import accuracy\n",
    "\n",
    "# Quantization:\n",
    "from enot.quantization import TrtFakeQuantizedModel\n",
    "from enot.quantization import DefaultQuantizationDistiller\n",
    "\n",
    "# TensorRT inference:\n",
    "from enot_lite import backend\n",
    "from enot_lite.calibration import CalibrationTableTensorrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model evaluation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function can evaluate both nn.Modules and executable functions.\n",
    "def eval_model(model_fn, dataloader):\n",
    "\n",
    "    if isinstance(model_fn, nn.Module):\n",
    "        model_fn.eval()\n",
    "\n",
    "    total = 0\n",
    "    total_accuracy = 0.0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader):\n",
    "\n",
    "            n = inputs.shape[0]\n",
    "\n",
    "            pred_labels = model_fn(inputs)\n",
    "            batch_loss = criterion(pred_labels, labels)\n",
    "            batch_accuracy = accuracy(pred_labels, labels)\n",
    "\n",
    "            total += n\n",
    "            total_loss += batch_loss.item() * n\n",
    "            total_accuracy += batch_accuracy.item() * n\n",
    "\n",
    "    return total_loss / total, total_accuracy / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### In the following cell we setup all necessary dirs\n",
    "\n",
    "* `ENOT_HOME_DIR` - ENOT framework home directory\n",
    "* `ENOT_DATASETS_DIR` - root directory for datasets (imagenette2, ...)\n",
    "* `PROJECT_DIR` - project directory to save training logs, checkpoints, ...\n",
    "* `ONNX_MODEL_PATH` - onnx model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENOT_HOME_DIR = Path.home() / '.enot'\n",
    "ENOT_DATASETS_DIR = ENOT_HOME_DIR / 'datasets'\n",
    "PROJECT_DIR = ENOT_HOME_DIR / 'enot-lite_quantization'\n",
    "ONNX_MODEL_PATH = PROJECT_DIR / 'mobilenetv2.onnx'\n",
    "\n",
    "ENOT_HOME_DIR.mkdir(exist_ok=True)\n",
    "ENOT_DATASETS_DIR.mkdir(exist_ok=True)\n",
    "PROJECT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset and create dataloaders\n",
    "\n",
    "We will use Imagenet-10k dataset in this example.\n",
    "\n",
    "Imagenet-10k dataset is a subsample of [Imagenet](https://image-net.org/challenges/LSVRC/index.php) dataset. It contains 5000 training images and 5000 validation images. Training images are uniformly gathered from the original training set, and validation images are gathered from the original validation set, 5 per each class.\n",
    "\n",
    "`create_imagenet10k_dataloaders` function prepares datasets for you in this example; specifically, it:\n",
    "1. downloads and unpacks dataset into `ENOT_DATASETS_DIR`;\n",
    "1. creates and returns train and validation dataloaders.\n",
    "\n",
    "The two parts of the dataset:\n",
    "* train: for quantization procedure (`ENOT_DATASETS_DIR`/imagenet10k/train/)\n",
    "* validation: for model validation (`ENOT_DATASETS_DIR`/imagenet10k/val/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, validation_dataloader = create_imagenet10k_dataloaders(\n",
    "    dataset_root_dir=ENOT_DATASETS_DIR,\n",
    "    input_size=224,\n",
    "    batch_size=50,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate pretrained MobileNetV2 from torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.mobilenetv2 import mobilenet_v2\n",
    "regular_model = mobilenet_v2(pretrained=True).cuda()\n",
    "regular_model.classifier[0].p = 0.0  # This is required to stabilize fine-tuning procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_accuracy = eval_model(regular_model, validation_dataloader)\n",
    "print(f'Regular (non-quantized) model: accuracy={val_accuracy:.3f}, loss={val_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End2end quantization with ENOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply run our ``DefaultQuantizationDistiller`` class to use distillation with quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_quantized_model = TrtFakeQuantizedModel(regular_model).cuda()\n",
    "\n",
    "# Distill model quantization thresholds and weights using RMSE loss.\n",
    "dist = DefaultQuantizationDistiller(\n",
    "    quantized_model=fake_quantized_model,\n",
    "    dataloader=train_dataloader,\n",
    "    device='cuda',\n",
    "    logdir=PROJECT_DIR,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "# Uncomment lines below if you want to reach the best quantization\n",
    "# performance (71.90% top1 accuracy for quantized model).\n",
    "\n",
    "# dist.distillers[0].n_epochs = 10  # Increase the number of threshold fine-tuning epochs.\n",
    "# dist.distillers[0].scheduler.T_max *= 10  # Fix learning rate schedule.\n",
    "\n",
    "dist.distill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_quantized_model.enable_quantization_mode(True)\n",
    "val_loss, val_accuracy = eval_model(fake_quantized_model, validation_dataloader)\n",
    "print(f'Optimized quantized model: accuracy={val_accuracy:.3f}, loss={val_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using enot-lite with TensorRT int8 backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **enot-lite**, we should export our quantized model to onnx, and save its calibration table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_quantized_model.cpu()\n",
    "fake_quantized_model.export_to_onnx(\n",
    "    torch.zeros(50, 3, 224, 224),\n",
    "    'exported_model.onnx',\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize **enot-lite** inference session with TensorRT Int8 Execution Provider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()  # Empty PyTorch CUDA cache before running enot-lite.\n",
    "\n",
    "calibration_table = CalibrationTableTensorrt.from_file_json(\n",
    "    './exported_model.onnx.calibration_table_enot_lite'\n",
    ")\n",
    "sess = backend.OrtTensorrtInt8Backend('./exported_model.onnx', calibration_table)\n",
    "input_name = sess.get_inputs()[0].name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First TensorRT run is usually slow because it chooses the best algorithms for inference.\n",
    "\n",
    "Let's run session once before validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess.run(output_names=None, input_feed={input_name: np.zeros((50, 3, 224, 224), dtype=np.float32)});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate quantized model on TensorRT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(inputs):\n",
    "    input_feed = {input_name: inputs.cpu().numpy()}\n",
    "    trt_output = sess.run(output_names=None, input_feed=input_feed)[0]\n",
    "    return torch.tensor(trt_output, device='cuda')\n",
    "\n",
    "val_loss, val_accuracy = eval_model(model_fn, validation_dataloader)\n",
    "print(f'Quantized model with fine-tuned weights with TRT: accuracy={val_accuracy:.3f}, loss={val_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 3000
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
