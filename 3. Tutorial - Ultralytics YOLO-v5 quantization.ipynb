{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fda310e",
   "metadata": {},
   "source": [
    "## Automatic quantization and optimized inference for YOLOv5 with ONNX Runtime (TensorrRT Execution Provider)\n",
    "\n",
    "This notebook demonstrates simple procedure for Ultralytics YOLOv5 quantization.\n",
    "\n",
    "Our quantization process consists of quantized model calibration, quantization thresholds adjustment and weight fine-tuning using distillation. Finally, we demonstrate inference of our quantized model using ONNX Runtime framework.\n",
    "\n",
    "### Main chapters of this notebook:\n",
    "1. Setup the environment\n",
    "1. Prepare dataset and create dataloaders\n",
    "1. Baseline YOLOv5 ONNX creation\n",
    "1. Quantize YOLOv5\n",
    "1. Measure speed of default YOLOv5 inferenced via default PyTorch and quantized YOLOv5 inferenced via ONNX Runtime (TensorRT)\n",
    "1. Measure mAP for float and quantized versions\n",
    "\n",
    "Before running this example make sure that TensorRT supports your GPU for INT8 inference  (``cuda compute capability`` > 6.1, as described [here](https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html#hardware-precision-matrix))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2d3e9b",
   "metadata": {},
   "source": [
    "## Setup the environment\n",
    "\n",
    "First, let's set up the environment and make some common imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ea5d0b-f6a3-43c0-a3e7-1338beb1ef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install 'numpy<1.24'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7967df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to uncomment and change this variable to match free GPU index\n",
    "# %env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae062b0",
   "metadata": {},
   "source": [
    "1. Install enot-autodl and ONNX Runtime libraries and create jupyter kernel with them.\n",
    "2. Clone specific commit from YOLOv5 repository: https://github.com/ultralytics/yolov5/commit/f76a78e7078185ecdc67470d8658103cf2067c81\n",
    "3. Replace the val.py script with our val.py\n",
    "4. Replace path to COCO dataset folder in 'yolov5/data/coco.yaml' file. If you do not have pre-downloaded MS COCO dataset - you can leave it as is and the dataset will be automatically downloaded.\n",
    "\n",
    "Steps 2 and 3 will be done with these commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d02fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/ultralytics/yolov5\n",
    "! cd yolov5/ && git checkout f76a78e7078185ecdc67470d8658103cf2067c81\n",
    "! cp tutorial_utils/val.py yolov5/val.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d93b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('yolov5/')\n",
    "\n",
    "import itertools\n",
    "import statistics\n",
    "import numpy as np\n",
    "from timeit import Timer\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim import RAdam\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "# quantization procedure\n",
    "from enot.quantization import TensorRTFakeQuantizedModel\n",
    "from enot.quantization import calibrate\n",
    "from enot.quantization import distill\n",
    "from enot.quantization import RMSELoss\n",
    "\n",
    "# optimized inference\n",
    "from tutorial_utils.inference import create_onnxruntime_session\n",
    "import onnxsim\n",
    "\n",
    "# converters from onnx to pytorch\n",
    "from onnx2torch import convert\n",
    "from onnx2torch.utils.custom_export_to_onnx import OnnxToTorchModuleWithCustomExport\n",
    "\n",
    "# dataset creation functions\n",
    "from yolov5.utils.dataloaders import create_dataloader\n",
    "from yolov5.utils.general import check_dataset\n",
    "\n",
    "# function for loading yolo checkpoint\n",
    "from yolov5.models.experimental import attempt_load\n",
    "\n",
    "# onnx conversion function\n",
    "from yolov5.export import export_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad45e20",
   "metadata": {},
   "source": [
    "### In the following cell we setup all necessary contents\n",
    "\n",
    "* `HOME_DIR` - experiments home directory\n",
    "* `PROJECT_DIR` - project directory to save training logs, checkpoints, ...\n",
    "* `ONNX_MODEL_PATH` - onnx model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb6f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = Path.home() / '.optimization_experiments'\n",
    "DATASETS_DIR = HOME_DIR / 'datasets/coco_for_yolo'\n",
    "PROJECT_DIR = HOME_DIR / 'yolov5s_quantization'\n",
    "QUANT_ONNX_PATH = './yolov5s_trt_int8.onnx'\n",
    "ONNX_PATH = './yolov5s.onnx'\n",
    "\n",
    "HOME_DIR.mkdir(exist_ok=True)\n",
    "PROJECT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "IMG_SIZE = 640\n",
    "IMG_SHAPE = (BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f2ec45",
   "metadata": {},
   "source": [
    "## Prepare dataset and create dataloaders\n",
    "\n",
    "We will use MS COCO dataset in this example.\n",
    "\n",
    "\n",
    "`create_dataloader` and `check_dataset` functions prepare datasets for you in this example; specifically, it:\n",
    "1. downloads and unpacks dataset into folder pointed out in `yolov5/data/coco.yaml`;\n",
    "1. creates and returns train and validation dataloaders.\n",
    "\n",
    "**IMPORTANT NOTE**: since this is example notebook we will train and validate model in **THE SAME DATASET**. For better performance and generalization use separate dataset for train and val procedure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4073b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7cc37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('yolov5/data/coco.yaml', 'r') as f:\n",
    "    coco_cfg = yaml.load(f, yaml.Loader)\n",
    "\n",
    "coco_cfg['path'] = DATASETS_DIR.as_posix()\n",
    "\n",
    "with open('yolov5/data/coco.yaml', 'w') as f:\n",
    "    yaml.dump(coco_cfg, f)\n",
    "\n",
    "data = check_dataset('yolov5/data/coco.yaml', autodownload=True)\n",
    "\n",
    "valid_dataloader = create_dataloader(data[\"val\"], IMG_SIZE, BATCH_SIZE, 32, False, pad=0.5, rect=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e874892",
   "metadata": {},
   "source": [
    "## Baseline YOLO-v5 onnx creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2faa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the default YOLOv5 model contains conditional execution ('if' nodes), we have to save\n",
    "# it to ONNX format and convert back to PyTorch to perform quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297c08c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run yolov5/export.py --weights=yolov5s.pt --include=onnx --batch-size={BATCH_SIZE} --imgsz={IMG_SIZE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5837bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_model = convert(ONNX_PATH).cuda()\n",
    "regular_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d4eea0",
   "metadata": {},
   "source": [
    "## Quantization YOLO-v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be645c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define function for converting dataset samples to model inputs.\n",
    "\n",
    "\n",
    "def sample_to_model_inputs(x):\n",
    "    # x[0] is the first item from dataloader sample. Sample is a tuple where 0'th element is a tensor with images.\n",
    "    x = x[0]\n",
    "\n",
    "    # Model is on CUDA, so input images should also be on CUDA.\n",
    "    x = x.cuda()\n",
    "\n",
    "    # Converting tensor from int8 to float data type.\n",
    "    x = x.float()\n",
    "\n",
    "    # YOLOv5 image normalization (0-255 to 0-1 normalization)\n",
    "    x /= 255\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31e8835-aec8-458c-bc26-05239374b38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please consider to specify `quantization_scheme` for `TensorRTFakeQuantizedModel`,\n",
    "# quantization scheme can affect the perfomance of the quantized model.\n",
    "# See for details: https://enot-autodl.rtd.enot.ai/en/stable/reference_documentation/quantization.html#enot.quantization.TensorRTFakeQuantizedModel\n",
    "\n",
    "fake_quantized_model = TensorRTFakeQuantizedModel(regular_model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3604b6d1-afd5-4b18-af92-7e99c935d58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate quantization thresholds using 10 batches.\n",
    "# Note that we are using **valid_dataloader** for fast calculation.\n",
    "# For real purpose you have to use your train data, at least some part of it.\n",
    "\n",
    "with torch.no_grad(), calibrate(fake_quantized_model):\n",
    "    for batch in itertools.islice(valid_dataloader, 10):\n",
    "        batch = sample_to_model_inputs(batch)\n",
    "        fake_quantized_model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distill model quantization thresholds and weights using RMSE loss.\n",
    "# Note that we are using **valid_dataloader** for fast calculation.\n",
    "# For real purpose you have to use your train data, at least some part of it.\n",
    "\n",
    "n_epochs = 5\n",
    "with distill(fq_model=fake_quantized_model, tune_weight_scale_factors=True) as (qdistill_model, params):\n",
    "    optimizer = RAdam(params=params, lr=0.005, betas=(0.9, 0.95))\n",
    "    scheduler = CosineAnnealingLR(optimizer=optimizer, T_max=len(valid_dataloader) * n_epochs)\n",
    "    distillation_criterion = RMSELoss()\n",
    "\n",
    "    for _ in range(n_epochs):\n",
    "        for batch in (tqdm_it := tqdm(valid_dataloader)):\n",
    "            batch = sample_to_model_inputs(batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss: torch.Tensor = torch.tensor(0.0).cuda()\n",
    "            for student_output, teacher_output in qdistill_model(batch):\n",
    "                loss += distillation_criterion(student_output, teacher_output)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            tqdm_it.set_description(f'loss: {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008bb58c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fake_quantized_model.cuda()\n",
    "fake_quantized_model.enable_quantization_mode(True)\n",
    "fake_quantized_model.cpu()\n",
    "\n",
    "torch.onnx.export(\n",
    "    model=fake_quantized_model,\n",
    "    args=torch.ones(*IMG_SHAPE),\n",
    "    f=QUANT_ONNX_PATH,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    opset_version=13,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2382f25f",
   "metadata": {},
   "source": [
    "## Speed measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f48f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cfc681",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov5s = attempt_load('yolov5s.pt').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b0cc45-236a-4784-aa96-7473a69793d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_fps(infer):\n",
    "    for _ in range(50):  # warmup\n",
    "        infer()\n",
    "\n",
    "    number = 50\n",
    "    measurements = Timer(infer).repeat(repeat=50, number=number)\n",
    "    norm = statistics.mean(measurements) / number / BATCH_SIZE\n",
    "    fps = 1.0 / norm\n",
    "    return fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1dee9c-ffad-4f77-ae21-966ee98f6a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.ones((BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE), dtype=torch.float32, device='cuda')\n",
    "\n",
    "\n",
    "def infer_torch():\n",
    "    yolov5s(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860f55c9-d347-479b-979d-e5fbb78c56f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "proto, _ = onnxsim.simplify(QUANT_ONNX_PATH)\n",
    "\n",
    "onnxruntime_sess = create_onnxruntime_session(\n",
    "    proto=proto,\n",
    "    input_sample=inputs,\n",
    "    output_shape=(BATCH_SIZE, 25200, 85),\n",
    ")\n",
    "\n",
    "\n",
    "def infer_onnxruntime():\n",
    "    onnxruntime_sess(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19d9e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_fps(infer_torch)  # PyTorch FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b83fe4-956a-47ca-9a21-47505f2140b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_fps(infer_onnxruntime)  # ONNX Runtime (TensorRT Execution Provider) FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d75d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e946a32e",
   "metadata": {},
   "source": [
    "## mAP evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c2c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common validation function for Ultralytics YOLO models\n",
    "from yolov5.val import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d325666",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {\n",
    "    'data': 'yolov5/data/coco.yaml',\n",
    "    'weights': 'yolov5s.pt',\n",
    "    'half': True,\n",
    "    'batch_size': 8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf53f3e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run(**opt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6318167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82038aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt['onnxruntime_sess'] = onnxruntime_sess\n",
    "opt['half'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff882f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(**opt);"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 3000
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
