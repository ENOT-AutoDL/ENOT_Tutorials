{"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"language_info":{"file_extension":".py","version":"3.7.7","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3"},"notebookId":"71515f4b-50d4-4239-9926-33a0b451e9b1"},"cells":[{"cell_type":"markdown","source":"## Using latency optimization with custom operations\n\nHere we describe the additional steps required to enable latency optimization in your model.\n\n### Notebook consists of next main stages:\n1. Setup the environment\n1. Add a custom non searchable operation/module with latency to use with search space\n1. Add a custom searchable operation with latency to use with search space\n1. Build model with custom operation\n1. Check pretrain search and tune phases","metadata":{"cellId":"syjov56di3e9ote1ugep"}},{"cell_type":"markdown","source":"## 1. Setup the environment\nFirst, let's set up the environment and common imports.","metadata":{"cellId":"ql7u7j3ftwbbfvprjvtv9p"}},{"cell_type":"code","source":"import sys\n\nsys.path.append('ENOT_Tutorials')","metadata":{"cellId":"gk8hnkn2rgb27td3p37ath"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\n\nimport torch\nimport torch.nn as nn\n\nfrom torch.optim import SGD\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch_optimizer import RAdam\nfrom torchvision.models.mobilenet import ConvBNReLU\n\nfrom enot.models import BaseSearchableOperation\nfrom enot.models import build_simple_block_model\nfrom enot.models import register_searchable_op\nfrom enot.models import SearchSpaceModel\nfrom enot.phases import pretrain\nfrom enot.phases import search\nfrom enot.phases import train\nfrom enot.utils.latency import conv_mac_count\nfrom enot.utils.latency import LatencyMixin\n\nfrom enot_utils.metric_utils import accuracy\nfrom enot_utils.schedulers import WarmupScheduler\n\nfrom tutorial_utils.dataset import create_imagenette_dataloaders","metadata":{"cellId":"f8oatxfkxkrgjaiz18hcsa"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Add a custom non searchable operation/module with latency to use with search space\n\nTo add latency support to your custom regular operation/module, you need to implement latency calculation.\n\nAdding latency calculation is done in two steps:\n1. Make your class a child of `enot.utils.latency.LatencyMixin`. `LatencyMixin` offer interface of latency calculation for `SearchSpaceModel` using user defined methods of latency calculations (see next step).\n2. Add a method with a signature `latency_<name>(self, spatial_size) -> float`, which calculates latency.\n\nAt this moment only `'mmac'` (millions of multiply-accumulates) is supported.","metadata":{"cellId":"9osfpimb7fb7ycnrxtxhkk"}},{"cell_type":"markdown","source":"### Head and stem of your model","metadata":{"cellId":"wwon05pbuu8np4ucguzao"}},{"cell_type":"code","source":"class MyStem(ConvBNReLU, LatencyMixin):\n    def __init__(\n            self,\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=1,\n            groups=1,\n            norm_layer=None,\n    ):\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.groups = groups\n        \n        super().__init__(in_channels, out_channels, kernel_size, stride, groups, norm_layer)\n    \n    def latency_mmac(self, spatial_size):\n        \"\"\"Calculate millions of multiply-accumulates\"\"\"\n        mmac, _ = conv_mac_count(\n            spatial_size=spatial_size,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n            in_channels=self.in_channels,\n            padding=0,\n            out_channels=self.out_channels,\n            groups=self.groups,\n        )\n        \n        return mmac\n    \n    \nclass MyHead(nn.Sequential, LatencyMixin):\n    def __init__(\n        self, \n        in_channels,\n        hidden_channels,\n        num_classes,\n        dropout_rate=0.2\n    ):\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.num_classes = num_classes\n        \n        super().__init__(\n            ConvBNReLU(in_channels, hidden_channels, kernel_size=1),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(hidden_channels, num_classes)\n        )\n      \n    def latency_mmac(self, spatial_size):\n        \"\"\"Calculate millions of multiply-accumulates\"\"\"\n        mmac, (h_out, w_out) = conv_mac_count(spatial_size, 1, 1, self.in_channels, 0, self.hidden_channels)\n        mmac += h_out * w_out * self.hidden_channels / 10**6\n        mmac += self.hidden_channels * self.num_classes / 10**6\n        \n        return mmac","metadata":{"cellId":"686kdowye1qn372epz211j"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Add a custom searchable operation with latency to use with search space\n\nWe will re-implement our custom operation with latency support. All details of adding custom operation can be found in \"Tutorial - adding custom ops\".\n\n**IMPORTANT**:<br>\n`BaseSearchableOperation` is child of `LatencyMixin`, so you only need to add a method with a signature `latency_<name>(self, spatial_size) -> float` which calculates latency.","metadata":{"cellId":"k2iwc9fjh4bb8158owl7k"}},{"cell_type":"code","source":"# Define short parameter parsing rules\n# Format: {short_param_name: (original_param_name, parser)}\nshort_args = {\n    'k': ('kernel_size', int),\n}\n\n\n@register_searchable_op('MyOp', short_args)\nclass MyOperation(BaseSearchableOperation):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        stride,\n        kernel_size=3,\n        padding=None,   \n        use_skip_connection=True\n    ):\n        super().__init__(\n            in_channels,\n            out_channels,\n            use_skip_connection,\n        )\n            \n        if padding is None:\n            padding = (kernel_size - 1) // 2\n        \n        self.stride = stride\n        self.kernel_size = kernel_size\n        self.padding = padding\n        \n        self.operation = nn.Sequential(\n            nn.Conv2d(\n                in_channels=in_channels, \n                out_channels=out_channels, \n                kernel_size=kernel_size, \n                stride=stride, \n                padding=padding,\n            ),\n            nn.ReLU(),\n            nn.BatchNorm2d(out_channels)\n        )\n\n    def get_last_batch_norm(self) -> nn.BatchNorm2d:\n        return self.operation[-1]\n\n    def replace_last_batch_norm(self, new_last_batch_norm: nn.BatchNorm2d) -> None:\n        self.operation[-1] = new_last_batch_norm\n        \n    def operation_forward(self, x):\n        return self.operation(x)\n        \n    def latency_mmac(self, spatial_size):\n        \"\"\"Calculate number of millions of multiply-accumulate operations in MyOperation\"\"\"\n        \n        def num_conv_steps(size, padding, kernel, stride):\n            size = size + 2 * padding - (kernel - 1)\n            return (size + stride - 1) // stride\n\n        h, w = spatial_size\n        h_steps = num_conv_steps(h, self.padding, self.kernel_size, self.stride)\n        w_steps = num_conv_steps(w, self.padding, self.kernel_size, self.stride)\n        mmac = h_steps * w_steps * self.kernel_size**2 * self.in_channels * self.out_channels\n        mmac /= 10**6\n        \n        return mmac","metadata":{"cellId":"3yw27wj8scjnuolca12opa"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Build model with custom operation","metadata":{"cellId":"qu6ob5avtboqmp1oong9h"}},{"cell_type":"code","source":"SEARCH_OPS = [\n    'MIB_k=3_t=6',\n    'MIB_k=5_t=6',\n    'MyOp_k=3',\n]\n\nblocks_in_channels = 32\nblocks_out_channels = 320\nhead_hidden_channels = 1280\nnum_classes = 10\n\n# build model\nmodel = build_simple_block_model(\n    in_channels=blocks_in_channels,\n    search_ops=SEARCH_OPS,\n    blocks_out_channels=[24, 32, 64, 96, 160, blocks_out_channels],\n    blocks_count=[2, 2, 2, 1, 2, 1],\n    blocks_stride=[2, 2, 2, 1, 2, 1],\n    stem=MyStem(in_channels=3, out_channels=blocks_in_channels, stride=2),\n    head=MyHead(in_channels=blocks_out_channels, hidden_channels=head_hidden_channels, num_classes=num_classes),\n)\n# move model to search space\nsearch_space = SearchSpaceModel(model).cuda()","metadata":{"cellId":"xb1ov8xrhtem3q3ak7hru"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Check pretrain search and tune phases\n\nLet's check that everything works.\n\n**IMPORTANT**:<br>\nTo turn on latency optimization you must set `latency_loss_weight` (> 0) parameter of `search`. ","metadata":{"cellId":"1csnmc6n2st98aa2ssv86b"}},{"cell_type":"code","source":"ENOT_HOME_DIR = Path.home() / '.enot'\nENOT_DATASETS_DIR = ENOT_HOME_DIR / 'datasets'\nPROJECT_DIR = ENOT_HOME_DIR / 'using_latency_optimization'\n\nENOT_HOME_DIR.mkdir(exist_ok=True)\nENOT_DATASETS_DIR.mkdir(exist_ok=True)\nPROJECT_DIR.mkdir(exist_ok=True)","metadata":{"cellId":"el8nso4207e5yt2769d5ls"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataloaders = create_imagenette_dataloaders(\n    dataset_root_dir=ENOT_DATASETS_DIR, \n    project_dir=PROJECT_DIR,\n    input_size=(224, 224),\n    batch_size=32,\n    imagenette_kind='imagenette2-320',\n)","metadata":{"cellId":"1meetj04khp758dmaoncja"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# define directory for text logs and tensorboard logs\npretrain_dir = PROJECT_DIR / 'pretrain'\npretrain_dir.mkdir(exist_ok=True)\n\nN_EPOCHS = 3\nN_WARMUP_EPOCHS = 1\nlen_train = len(dataloaders['pretrain_train_dataloader'])\n\noptimizer = SGD(params=search_space.model_parameters(), lr=0.06, momentum=0.9, weight_decay=1e-4)\nscheduler = CosineAnnealingLR(optimizer, T_max=len_train*N_EPOCHS, eta_min=1e-8)\nscheduler = WarmupScheduler(scheduler, warmup_steps=len_train*N_WARMUP_EPOCHS)\nloss_function = nn.CrossEntropyLoss().cuda()\n\npretrain(\n    search_space=search_space,\n    exp_dir=pretrain_dir,\n    train_loader=dataloaders['pretrain_train_dataloader'],\n    valid_loader=dataloaders['pretrain_validation_dataloader'],\n    optimizer=optimizer,\n    scheduler=scheduler,\n    metric_function=accuracy,\n    loss_function=loss_function,\n    epochs=N_EPOCHS,\n)","metadata":{"scrolled":true,"cellId":"6jag5cjykqn19p0lz7vtxy"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# define directory for text logs and tensorboard logs\nsearch_dir = PROJECT_DIR / 'search'\nsearch_dir.mkdir(exist_ok=True)\n\noptimizer = RAdam(search_space.architecture_parameters(), lr=0.01)\n\nsearch(\n    search_space=search_space,\n    exp_dir=search_dir,\n    search_loader=dataloaders['search_train_dataloader'],\n    valid_loader=dataloaders['search_validation_dataloader'],\n    optimizer=optimizer,\n    loss_function=loss_function,\n    metric_function=accuracy,\n    latency_loss_weight=2.0e-3,\n    epochs=3,\n)","metadata":{"scrolled":true,"cellId":"u5js609xuf28mks9g7xs1"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# get regular model with best architecture\nbest_model = search_space.get_network_with_best_arch().cuda()","metadata":{"cellId":"eatsdozrhbu9kfji2cpwf"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# define directory for text logs and tensorboard logs\ntune_dir = PROJECT_DIR / 'tune'\ntune_dir.mkdir(exist_ok=True)\n\noptimizer = RAdam(best_model.parameters(), lr=5e-3, weight_decay=4e-5)\n\ntrain(\n    model=best_model,\n    exp_dir=tune_dir,\n    train_loader=dataloaders['tune_train_dataloader'],\n    valid_loader=dataloaders['tune_validation_dataloader'],\n    optimizer=optimizer,\n    loss_function=loss_function,\n    metric_function=accuracy,\n    epochs=3,\n)","metadata":{"cellId":"wmc8ir4074lycs4u2cmbf"},"outputs":[],"execution_count":null}]}