{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with NAS\n",
    "\n",
    "This notebook describes the basic steps you need to optimize an architecture using NAS framework.\n",
    "\n",
    "### Main chapters of this notebook:\n",
    "1. Setup the environment\n",
    "1. Prepare dataset and create dataloaders\n",
    "1. Create model and move it into search space\n",
    "1. Pretrain constructed search space\n",
    "1. Search the best architecture\n",
    "1. Tune model with the best architecture\n",
    "1. Make model portable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the environment\n",
    "First, let's set up the environment and make some common imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "# You may need to uncomment and change this variable to match free GPU index\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch_optimizer import RAdam\n",
    "\n",
    "from enot.utils.common import make_portable\n",
    "from enot.models import SearchSpaceModel\n",
    "from enot.models.mobilenet import build_mobilenet\n",
    "from enot.optimize import PretrainOptimizer\n",
    "from enot.optimize import SearchOptimizer\n",
    "from enot.latency import current_latency\n",
    "from enot.latency import initialize_latency\n",
    "from enot.latency import min_latency\n",
    "from enot.latency import max_latency\n",
    "from enot.latency import mean_latency\n",
    "\n",
    "from tutorial_utils.train import accuracy\n",
    "from tutorial_utils.train import WarmupScheduler\n",
    "\n",
    "from tutorial_utils.checkpoints import download_getting_started_pretrain_checkpoint\n",
    "from tutorial_utils.dataset import create_imagenette_dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the following cell we setup all necessary dirs\n",
    "\n",
    "* `HOME_DIR` - experiments home directory\n",
    "* `DATASETS_DIR` - root directory for datasets (imagenette2, ...)\n",
    "* `PROJECT_DIR` - project directory to save training logs, checkpoints, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = Path.home() / '.optimization_experiments'\n",
    "DATASETS_DIR = HOME_DIR / 'datasets'\n",
    "PROJECT_DIR = HOME_DIR / 'getting_started'\n",
    "\n",
    "HOME_DIR.mkdir(exist_ok=True)\n",
    "DATASETS_DIR.mkdir(exist_ok=True)\n",
    "PROJECT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset and create dataloaders\n",
    "\n",
    "We will use dataset Imagenette2 in this example. <br>\n",
    "`create_imagenette_dataloaders` function prepares datasets for you in this example; specifically, it:\n",
    "1. downloads and unpacks dataset into `DATASETS_DIR`;\n",
    "1. splits dataset into 4 parts, and saves annotations of every part in `PROJECT_DIR` (We need this to prevent the overfitting effect, but for big datasets you can split dataset as usual train/val/test);\n",
    "1. creates dataloaders for every stage, and returns dataloaders as a dictionary.\n",
    "\n",
    "The four parts of the dataset:\n",
    "* train: for pretrain and tuning stages (`PROJECT_DIR`/train.csv)\n",
    "* validation: to choose checkpoint for architecture optimization (`PROJECT_DIR`/validation.csv)\n",
    "* search: for architecture search stage (`PROJECT_DIR`/search.csv)\n",
    "* test: hold-out data for testing (`PROJECT_DIR`/test.csv)\n",
    "\n",
    "Available dataloaders in the dictionary:\n",
    "* pretrain_train_dataloader:\n",
    "    training dataloader for \"pretrain\" stage (using data from `PROJECT_DIR`/train.csv)\n",
    "\n",
    "* pretrain_validation_dataloader:\n",
    "    validation dataloader for \"pretrain\" stage (using data from `PROJECT_DIR`/validation.csv)\n",
    "\n",
    "* search_train_dataloader:\n",
    "    training dataloader for \"search\" stage (using data from `PROJECT_DIR`/search.csv)\n",
    "\n",
    "* search_validation_dataloader:\n",
    "    validation dataloader for \"search\" stage (using data from `PROJECT_DIR`/search.csv)\n",
    "\n",
    "* tune_train_dataloader:\n",
    "    training dataloader for \"finetune\" stage (same as pretrain_train_dataloader)\n",
    "\n",
    "* tune_validation_dataloader:\n",
    "    validation dataloader for \"finetune\" stage (same as pretrain_validation_dataloader)\n",
    "\n",
    "**NOTE:**<br>\n",
    "CSV annotations use the following format:\n",
    "```\n",
    "filepath,label\n",
    "<relative_path_1>,<int_label_1>\n",
    "<relative_path_2>,<int_label_2>\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = create_imagenette_dataloaders(\n",
    "    dataset_root_dir=DATASETS_DIR,\n",
    "    project_dir=PROJECT_DIR,\n",
    "    input_size=(224, 224),\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model and move it into search space\n",
    "\n",
    "Our architecture optimization procedure selects the best combination of operations from a user-defined search space. The easiest way to define one is to take a base architecture and add similar operations with different parameters: kernel size, expansion ratio, activation, etc. You can also add different kinds of operations or implement your own (see <span style=\"color:green;white-space:nowrap\">***10. Tutorial - adding a custom operation for model builder***</span>).\n",
    "\n",
    "In this example, we took MobileNet v2 as a base architecture and made a search space from MobileNet inverted bottleneck blocks.\n",
    "\n",
    "First, let's define a model for an image classification task. MobileNet-like models can be built by `build_mobilenet` function. `build_mobilenet` function returns a model with the following structure: (inputs) -> stem -> blocks -> head -> (outputs). Stem and head have a fixed structure, while blocks consist of user-defined operations. The template class follows MobileNet v2 structure, which can be found in [torchvision library](https://github.com/pytorch/vision/blob/master/torchvision/models/mobilenetv2.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search space will have these ops as choose options in each layer.\n",
    "# Short format for operations is 'Name_param1=value1_param2=value2...'.\n",
    "# MIB is a MNv2 inverted bottleneck, k is a kernel size for depthwise\n",
    "# convolution, and t is an expansion ratio coefficient.\n",
    "# See more in-depth info in \"10. Tutorial - adding a custom operation for model builder\".\n",
    "SEARCH_OPS = [\n",
    "    'MIB_k=3_t=6',\n",
    "    'MIB_k=5_t=6',\n",
    "    'MIB_k=7_t=6',\n",
    "]\n",
    "\n",
    "# build model\n",
    "model = build_mobilenet(\n",
    "    search_ops=SEARCH_OPS,\n",
    "    num_classes=10,\n",
    "    blocks_out_channels=[24, 32, 64, 96, 160, 320],\n",
    "    blocks_count=[2, 2, 2, 1, 2, 1],\n",
    "    blocks_stride=[2, 2, 2, 1, 2, 1],\n",
    ")\n",
    "\n",
    "# move model to search space\n",
    "search_space = SearchSpaceModel(model).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain constructed search space\n",
    "\n",
    "\"Pretrain\" phase is the first phase of NAS procedure. In \"pretrain\" phase, we train user-defined search space with different architecture combinations to further compare their task performance in search phase. \n",
    "\n",
    "We offer `PretrainOptimizer` class (a wrapper for regular pytorch optimizer), which does all necessary \"pretrain\" magic. You should use `search_space` as the model and `PretrainOptimizer` as the optimizer in your train loop.\n",
    "\n",
    "Follow these steps to turn your train loop into enot \"pretrain\" loop:\n",
    "1. Pass `search_space.model_parameters()` to the optimizer instead of `search_space.parameters()`.\n",
    "1. Use `PretrainOptimizer` as model optimizer.\n",
    "1. Wrap model step with a closure, and send the closure into `pretrain_optimizer.step(...)` method as parameter. This is necessary because `PretrainOptimizer` does more than one step per batch. Alternatively, you can use `pretrain_optimizer.model_step(...)` in the combination with `pretrain_optimizer.step()` to make multiple gradient accumulations before the actual optimizer step.\n",
    "1. Call `search_space.sample_random_arch()` method on each validation step. This method samples a single architecture in the search space. This is necessary if you want measure the expectation of the search space performance, not the score of an individual random model.\n",
    "1. By default, `PretrainOptimizer` require one batch of train data to initialize optimizations, so you should run `search_space.initialize_output_distribution_optimization(...)` before first model step. You can disable optimization checking in `PretrainOptimizer` constructor (`check_recommended_optimizations` parameter), but it is not recomended.\n",
    "\n",
    "**IMPORTANT:**\n",
    "We set `N_EPOCHS`= 3 in this example to make tutorial execution faster. This is not enough for good pretrain quality, and you should set `N_EPOCHS`= 300 if you want to reproduce our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 3\n",
    "N_WARMUP_EPOCHS = 1\n",
    "\n",
    "train_loader = dataloaders['pretrain_train_dataloader']\n",
    "validation_loader = dataloaders['pretrain_validation_dataloader']\n",
    "\n",
    "# using `search_space.model_parameters()` as optimizable variables\n",
    "optimizer = SGD(params=search_space.model_parameters(), lr=0.06, momentum=0.9, weight_decay=1e-4)\n",
    "# using `PretrainOptimizer` as a default optimizer\n",
    "pretrain_optimizer = PretrainOptimizer(search_space=search_space, optimizer=optimizer)\n",
    "\n",
    "len_train_loader = len(train_loader)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=len_train_loader * N_EPOCHS, eta_min=1e-8)\n",
    "scheduler = WarmupScheduler(scheduler, warmup_steps=len_train_loader * N_WARMUP_EPOCHS)\n",
    "\n",
    "metric_function = accuracy\n",
    "loss_function = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    print(f'EPOCH #{epoch}')\n",
    "\n",
    "    search_space.train()\n",
    "    train_metrics_acc = {\n",
    "        'loss': 0.0,\n",
    "        'accuracy': 0.0,\n",
    "        'n': 0,\n",
    "    }\n",
    "    for inputs, labels in train_loader:\n",
    "        # By default, `PretrainOptimizer` requires one batch of train data to initialize optimizations,\n",
    "        # so you should run `search_space.initialize_output_distribution_optimization(...)` before the first\n",
    "        # model step. You can disable optimization checking in `PretrainOptimizer` constructor\n",
    "        # (`check_recommended_optimizations` parameter), but this is not recommended.\n",
    "        if not search_space.output_distribution_optimization_enabled:\n",
    "            search_space.initialize_output_distribution_optimization(inputs)\n",
    "\n",
    "        pretrain_optimizer.zero_grad()\n",
    "        # Wrapping model step and backward with closure.\n",
    "        # Alternatively, here is `pretrain_optimizer.model_step(...)` example usage for gradient accumulation:\n",
    "        #\n",
    "        # pretrain_optimizer.zero_grad()\n",
    "        # for inputs, labels in train_loader:\n",
    "        #\n",
    "        #     def closure():\n",
    "        #         ...\n",
    "        #\n",
    "        #     pretrain_optimizer.model_step(closure)\n",
    "        #     if (n + 1) % 10 == 0:\n",
    "        #         pretrain_optimizer.step()\n",
    "        #         pretrain_optimizer.zero_grad()\n",
    "        def closure():\n",
    "            pred_labels = search_space(inputs)\n",
    "            batch_loss = loss_function(pred_labels, labels)\n",
    "            batch_loss.backward()\n",
    "            batch_metric = metric_function(pred_labels, labels)\n",
    "\n",
    "            train_metrics_acc['loss'] += batch_loss.item()\n",
    "            train_metrics_acc['accuracy'] += batch_metric.item()\n",
    "            train_metrics_acc['n'] += 1\n",
    "\n",
    "        pretrain_optimizer.step(closure)\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    train_loss = train_metrics_acc['loss'] / train_metrics_acc['n']\n",
    "    train_accuracy = train_metrics_acc['accuracy'] / train_metrics_acc['n']\n",
    "\n",
    "    print('train metrics:')\n",
    "    print('  loss:', train_loss)\n",
    "    print('  accuracy:', train_accuracy)\n",
    "\n",
    "    # We are validating on one specific fixed architecture to avoid the effect of fluctuations\n",
    "    # in the values of the loss function and metrics.\n",
    "    arch_to_test = [0] * len(search_space.search_variants_containers)\n",
    "    test_model = search_space.get_network_by_indexes(arch_to_test)\n",
    "    test_model.eval()\n",
    "\n",
    "    validation_loss = 0\n",
    "    validation_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validation_loader:\n",
    "            pred_labels = test_model(inputs)\n",
    "            batch_loss = loss_function(pred_labels, labels)\n",
    "            batch_metric = metric_function(pred_labels, labels)\n",
    "\n",
    "            validation_loss += batch_loss.item()\n",
    "            validation_accuracy += batch_metric.item()\n",
    "\n",
    "    n = len(validation_loader)\n",
    "    validation_loss /= n\n",
    "    validation_accuracy /= n\n",
    "\n",
    "    print('validation metrics:')\n",
    "    print('  loss:', validation_loss)\n",
    "    print('  accuracy:', validation_accuracy)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We pretrained search space for 3 epochs in this example. In this cell, we are downloading\n",
    "# search space checkpoint, pretrained for 300 epochs (for demonstration purposes).\n",
    "checkpoint_path = PROJECT_DIR / 'getting_started_pretrain_checkpoint.pth'\n",
    "download_getting_started_pretrain_checkpoint(checkpoint_path)\n",
    "\n",
    "search_space.load_state_dict(\n",
    "    torch.load(checkpoint_path)['model'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search the best architecture\n",
    "Now, when you have a trained search space, you can run the \"search\" phase. The setup is similar to pretrain.\n",
    "\n",
    "Follow these steps to turn your train loop into enot \"search\" loop:\n",
    "Pass `search_space.architecture_parameters()` to the optimizer instead of `search_space.parameters()`.\n",
    "1. Use `SearchOptimizer` as model optimizer.\n",
    "1. Wrap model step with a closure, and send the closure into `search_optimizer.step(...)` method as parameter. Alternatively, you can use `search_optimizer.model_step(...)` in the combination with `search_optimizer.step()` to make multiple gradient accumulations before the actual optimizer step.\n",
    "1. You must call `search_optimizer.prepare_validation_model()` right before performing the validation. This will prepare a single architecture for validation. By default, the best so far is used, but you can also provide your own model for validation.\n",
    "\n",
    "Additionally, you can run optimization with respect to latency. To use latency optimization:\n",
    "\n",
    "1. Add latency loss to your main loss before calling backward. An example can be seen below.\n",
    "1. To initialize latency of all operations, use `initialize_latency` function from `enot.latency` package. Currently, we support `mmac` latency type and two third-party calculators for this latency type: `mmac.thop` and `mmac.pthflops`.\n",
    "\n",
    "Starting from version 2.5.0, <span style=\"color:green\">**you can also specify maximal latency value**</span>. This restricts search process to consider only models which latencies are smaller than the provided latency. For the details, see <span style=\"color:green;white-space:nowrap\">***6. Tutorial - search with the specified latency***</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "latency_loss_weight = 2e-3\n",
    "\n",
    "# Optimizing `search_space.architecture_parameters()`.\n",
    "optimizer = RAdam(search_space.architecture_parameters(), lr=0.01)\n",
    "\n",
    "# Using `SearchOptimizer` as a default optimizer.\n",
    "search_optimizer = SearchOptimizer(search_space, optimizer)\n",
    "\n",
    "metric_function = accuracy\n",
    "loss_function = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "train_loader = dataloaders['search_train_dataloader']\n",
    "validation_loader = dataloaders['search_validation_dataloader']\n",
    "\n",
    "latency_type = 'mmac'  # or mmac.thop, mmac.pthflops\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    print(f'EPOCH #{epoch}')\n",
    "\n",
    "    search_space.train()\n",
    "    train_metrics_acc = {\n",
    "        'loss': 0.0,\n",
    "        'accuracy': 0.0,\n",
    "        'n': 0,\n",
    "    }\n",
    "    for inputs, labels in train_loader:\n",
    "\n",
    "        # Initialize latency.\n",
    "        if latency_type and search_space.latency_type is None:\n",
    "            latency_container = initialize_latency(latency_type, search_space, (inputs,))\n",
    "            print(f'Constant latency = {latency_container.constant_latency}')\n",
    "            print(\n",
    "                f'Min, mean and max latencies of search space: '\n",
    "                f'{min_latency(latency_container)}, '\n",
    "                f'{mean_latency(latency_container)}, '\n",
    "                f'{max_latency(latency_container)}'\n",
    "            )\n",
    "\n",
    "        search_optimizer.zero_grad()\n",
    "\n",
    "        # Wrapping model step and backward with closure.\n",
    "        def closure():\n",
    "            pred_labels = search_space(inputs)\n",
    "            batch_loss = loss_function(pred_labels, labels)\n",
    "\n",
    "            # Adding latency loss to main loss.\n",
    "            if latency_loss_weight is not None and latency_loss_weight != 0:\n",
    "                batch_loss += search_space.loss_latency_expectation * latency_loss_weight\n",
    "\n",
    "            batch_loss.backward()\n",
    "            batch_metric = metric_function(pred_labels, labels)\n",
    "\n",
    "            train_metrics_acc['loss'] += batch_loss.item()\n",
    "            train_metrics_acc['accuracy'] += batch_metric.item()\n",
    "            train_metrics_acc['n'] += 1\n",
    "\n",
    "        search_optimizer.step(closure)\n",
    "\n",
    "    train_loss = train_metrics_acc['loss'] / train_metrics_acc['n']\n",
    "    train_accuracy = train_metrics_acc['accuracy'] / train_metrics_acc['n']\n",
    "    arch_probabilities = np.array(search_space.architecture_probabilities)\n",
    "\n",
    "    print('train metrics:')\n",
    "    print('  loss:', train_loss)\n",
    "    print('  accuracy:', train_accuracy)\n",
    "    print('  arch_probabilities:')\n",
    "    print(arch_probabilities)\n",
    "\n",
    "    search_space.eval()\n",
    "\n",
    "    # Selecting the best architecture for validation.\n",
    "    search_optimizer.prepare_validation_model()\n",
    "\n",
    "    validation_loss = 0\n",
    "    validation_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validation_loader:\n",
    "            pred_labels = search_space(inputs)\n",
    "            batch_loss = loss_function(pred_labels, labels)\n",
    "            batch_metric = metric_function(pred_labels, labels)\n",
    "\n",
    "            validation_loss += batch_loss.item()\n",
    "            validation_accuracy += batch_metric.item()\n",
    "\n",
    "    n = len(validation_loader)\n",
    "    validation_loss /= n\n",
    "    validation_accuracy /= n\n",
    "\n",
    "    print('validation metrics:')\n",
    "    print('  loss:', validation_loss)\n",
    "    print('  accuracy:', validation_accuracy)\n",
    "    if search_space.latency_type is not None:\n",
    "        # Getting latency of the best architecture.\n",
    "        latency = current_latency(search_space)\n",
    "        print('  latency:', latency)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune model with the best architecture\n",
    "Now we take our best architecture from search space, and create a regular model using it. Then we run finetune procedure (usual training loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get regular model with the best architecture.\n",
    "best_arch = search_space.forward_architecture\n",
    "best_model = search_space.get_network_by_indexes(best_arch).cuda()\n",
    "print(best_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "optimizer = SGD(best_model.parameters(), lr=2e-4, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = None\n",
    "metric_function = accuracy\n",
    "loss_function = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "train_loader = dataloaders['tune_train_dataloader']\n",
    "validation_loader = dataloaders['tune_validation_dataloader']\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    print(f'EPOCH #{epoch}')\n",
    "\n",
    "    best_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_labels = best_model(inputs)\n",
    "        batch_loss = loss_function(pred_labels, labels)\n",
    "        batch_loss.backward()\n",
    "        batch_metric = metric_function(pred_labels, labels)\n",
    "\n",
    "        train_loss += batch_loss.item()\n",
    "        train_accuracy += batch_metric.item()\n",
    "\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    n = len(train_loader)\n",
    "    train_loss /= n\n",
    "    train_accuracy /= n\n",
    "\n",
    "    print('train metrics:')\n",
    "    print('  loss:', train_loss)\n",
    "    print('  accuracy:', train_accuracy)\n",
    "\n",
    "    best_model.eval()\n",
    "    validation_loss = 0\n",
    "    validation_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validation_loader:\n",
    "            pred_labels = best_model(inputs)\n",
    "            batch_loss = loss_function(pred_labels, labels)\n",
    "            batch_metric = metric_function(pred_labels, labels)\n",
    "\n",
    "            validation_loss += batch_loss.item()\n",
    "            validation_accuracy += batch_metric.item()\n",
    "\n",
    "    n = len(validation_loader)\n",
    "    validation_loss /= n\n",
    "    validation_accuracy /= n\n",
    "\n",
    "    print('validation metrics:')\n",
    "    print('  loss:', validation_loss)\n",
    "    print('  accuracy:', validation_accuracy)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Make model portable\n",
    "Model will be free from our framework dependencies you can save it and load just with pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sampled model free from our package\n",
    "portable_model = make_portable(best_model).cuda()\n",
    "\n",
    "# You can save it and load without our package just with pytorch\n",
    "model_path = PROJECT_DIR / 'best_model.pth'\n",
    "torch.save(portable_model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Reload kernel and run next cells only with torch dependent packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "\n",
    "from tutorial_utils.dataset import create_imagenette_dataloaders\n",
    "from tutorial_utils.train import accuracy\n",
    "\n",
    "\n",
    "HOME_DIR = Path.home() / '.optimization_experiments'\n",
    "DATASETS_DIR = HOME_DIR / 'datasets'\n",
    "PROJECT_DIR = HOME_DIR / 'getting_started'\n",
    "model_path = PROJECT_DIR / 'best_model.pth'\n",
    "\n",
    "metric_function = accuracy\n",
    "loss_function = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "dataloaders = create_imagenette_dataloaders(\n",
    "    dataset_root_dir=DATASETS_DIR,\n",
    "    project_dir=PROJECT_DIR,\n",
    "    input_size=(224, 224),\n",
    "    batch_size=32,\n",
    ")\n",
    "validation_loader = dataloaders['tune_validation_dataloader']\n",
    "\n",
    "# load saved portable model\n",
    "best_model_enot_free = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Best model works without our package!\n",
    "best_model_enot_free.eval()\n",
    "validation_loss = 0\n",
    "validation_accuracy = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in validation_loader:\n",
    "        pred_labels = best_model_enot_free(inputs)\n",
    "        batch_loss = loss_function(pred_labels, labels)\n",
    "        batch_metric = metric_function(pred_labels, labels)\n",
    "\n",
    "        validation_loss += batch_loss.item()\n",
    "        validation_accuracy += batch_metric.item()\n",
    "\n",
    "n = len(validation_loader)\n",
    "validation_loss /= n\n",
    "validation_accuracy /= n\n",
    "\n",
    "print('validation metrics:')\n",
    "print('  loss:', validation_loss)\n",
    "print('  accuracy:', validation_accuracy)\n",
    "\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 3000
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
