{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom model\n",
    "\n",
    "This notebook describes ways to implement your own model.\n",
    "\n",
    "Typical use cases:\n",
    "1. creating models which can not be built by \"block models builders\" from ENOT framework;\n",
    "1. you already have your model, and you don't want to rewrite your code.\n",
    "\n",
    "### Main chapters of this notebook:\n",
    "1. Setup the environment\n",
    "1. Prepare dataset and create dataloaders\n",
    "1. Create model and move it into search space\n",
    "1. Check pretrain, search and tune phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the environment\n",
    "First, let's set up the environment and make some common imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "# You may need to change this variable to match free GPU index\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch_optimizer import RAdam\n",
    "\n",
    "from enot.models import SearchSpaceModel\n",
    "from enot.models.mobilenet import MobileNetBaseHead\n",
    "from enot.models.mobilenet import MobileNetBaseStem\n",
    "from enot.models.operations import SearchableMobileInvertedBottleneck\n",
    "from enot.models.operations import SearchableFuseableSkipConv\n",
    "from enot.models.operations import SearchVariantsContainer\n",
    "from enot.optimize import EnotPretrainOptimizer\n",
    "from enot.optimize import EnotSearchOptimizer\n",
    "\n",
    "from tutorial_utils.train import accuracy\n",
    "from tutorial_utils.train import WarmupScheduler\n",
    "\n",
    "from tutorial_utils.checkpoints import download_getting_started_pretrain_checkpoint\n",
    "from tutorial_utils.dataset import create_imagenette_dataloaders\n",
    "from tutorial_utils.phases import tutorial_pretrain_loop\n",
    "from tutorial_utils.phases import tutorial_search_loop\n",
    "from tutorial_utils.phases import tutorial_train_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the following cell we setup all necessary dirs\n",
    "\n",
    "* `ENOT_HOME_DIR` - ENOT framework home directory\n",
    "* `ENOT_DATASETS_DIR` - root directory for datasets (imagenette2, ...)\n",
    "* `PROJECT_DIR` - project directory to save training logs, checkpoints, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENOT_HOME_DIR = Path.home() / '.enot'\n",
    "ENOT_DATASETS_DIR = ENOT_HOME_DIR / 'datasets'\n",
    "PROJECT_DIR = ENOT_HOME_DIR / 'custom_model'\n",
    "\n",
    "ENOT_HOME_DIR.mkdir(exist_ok=True)\n",
    "ENOT_DATASETS_DIR.mkdir(exist_ok=True)\n",
    "PROJECT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset and create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = create_imagenette_dataloaders(\n",
    "    dataset_root_dir=ENOT_DATASETS_DIR, \n",
    "    project_dir=PROJECT_DIR,\n",
    "    input_size=(224, 224),\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model and move it into search space\n",
    "To create your own model, you should build it as normal PyTorch model, following these rules:\n",
    "1. Search options must be placed into `SearchVariantsContainer` module;\n",
    "1. All `SearchVariantsContainer`'s must have the same number of operations. Operations in different containers can be different. **This restriction will be removed in the future versions.**\n",
    "1. You can add any PyTorch operations into `SearchVariantsContainer`, if you do not need to account latency. Otherwise, you should use predefined oprations from `enot.model.operations`, or extend your operation from `LatencyMixin` (see \"Tutorial - using latency optimization\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining your model class\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stem = MobileNetBaseStem(\n",
    "            in_channels=3\n",
    "        )\n",
    "        self.body = nn.ModuleList([\n",
    "            # 3 blocks with 3 search options in each block \n",
    "            self.build_search_variants_1(16, 24, 2),\n",
    "            self.build_search_variants_1(24, 24, 1),\n",
    "            self.build_search_variants_1(24, 24, 1),\n",
    "            # 2 fixed blocks\n",
    "            self.build_mib_k3_e6(24, 32, 2),\n",
    "            self.build_mib_k3_e6(32, 32, 1),\n",
    "            # 3 blocks with 3 search options in each block\n",
    "            self.build_search_variants_1(32, 64, 2),\n",
    "            self.build_search_variants_1(64, 64, 1),\n",
    "            self.build_search_variants_1(64, 64, 1),\n",
    "            # 1 fixed block\n",
    "            self.build_mib_k3_e6(64, 96, 1),\n",
    "            # 2 blocks with 3 search options in each block\n",
    "            self.build_search_variants_2(96, 160, 2),\n",
    "            self.build_search_variants_2(160, 160, 1),\n",
    "            # 1 block with 3 search options\n",
    "            self.build_search_variants_2(160, 320, 1),\n",
    "        ])\n",
    "        self.head = MobileNetBaseHead(\n",
    "            bottleneck_channels=320,\n",
    "            last_channels=1280, \n",
    "            num_classes=10,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def build_search_variants_1(in_channels, out_channels, stride):\n",
    "        return SearchVariantsContainer([\n",
    "            SearchableMobileInvertedBottleneck(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=stride,\n",
    "                expand_ratio=6,\n",
    "            ),\n",
    "            SearchableMobileInvertedBottleneck(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=5,\n",
    "                stride=stride,\n",
    "                expand_ratio=6,\n",
    "            ),\n",
    "            SearchableMobileInvertedBottleneck(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=7,\n",
    "                stride=stride,\n",
    "                expand_ratio=3,\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_search_variants_2(in_channels, out_channels, stride):\n",
    "        return SearchVariantsContainer([\n",
    "            SearchableMobileInvertedBottleneck(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=stride,\n",
    "                expand_ratio=6,\n",
    "            ),\n",
    "            SearchableMobileInvertedBottleneck(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=5,\n",
    "                stride=stride,\n",
    "                expand_ratio=6,\n",
    "            ),\n",
    "            SearchableFuseableSkipConv(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                stride=stride,\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_mib_k3_e6(in_channels, out_channels, stride):\n",
    "        return SearchableMobileInvertedBottleneck(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=stride,\n",
    "                expand_ratio=6,\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "\n",
    "        for block in self.body:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "\n",
    "# move model to search space\n",
    "search_space = SearchSpaceModel(model).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check pretrain, search and tune phases\n",
    "\n",
    "In this tutorial we use the same pretrain/search/train loops as in \"Tutorial - getting started\".\n",
    "\n",
    "**IMPORTANT:**<br>\n",
    "We set `N_EPOCHS`= 3 in this example to make tutorial execution faster. This is not enough for good pretrain quality, and you should set `N_EPOCHS`>= 100 if you want to achieve good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 3\n",
    "N_WARMUP_EPOCHS = 1\n",
    "len_train = len(dataloaders['pretrain_train_dataloader'])\n",
    "\n",
    "optimizer = SGD(params=search_space.model_parameters(), lr=0.06, momentum=0.9, weight_decay=1e-4)\n",
    "enot_optimizer = EnotPretrainOptimizer(search_space=search_space, optimizer=optimizer)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=len_train*N_EPOCHS, eta_min=1e-8)\n",
    "scheduler = WarmupScheduler(scheduler, warmup_steps=len_train*N_WARMUP_EPOCHS)\n",
    "loss_function = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "tutorial_pretrain_loop(\n",
    "    epochs=N_EPOCHS,\n",
    "    search_space=search_space,\n",
    "    enot_optimizer=enot_optimizer,\n",
    "    metric_function=accuracy,\n",
    "    loss_function=loss_function,\n",
    "    train_loader=dataloaders['pretrain_train_dataloader'],\n",
    "    validation_loader=dataloaders['pretrain_validation_dataloader'],\n",
    "    scheduler=scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = RAdam(search_space.architecture_parameters(), lr=0.01)\n",
    "enot_optimizer = EnotSearchOptimizer(search_space=search_space, optimizer=optimizer)\n",
    "\n",
    "tutorial_search_loop(\n",
    "    epochs=5,\n",
    "    search_space=search_space,\n",
    "    enot_optimizer=enot_optimizer,\n",
    "    metric_function=accuracy,\n",
    "    loss_function=loss_function,\n",
    "    train_loader=dataloaders['search_train_dataloader'],\n",
    "    validation_loader=dataloaders['search_validation_dataloader'],\n",
    "    latency_loss_weight=1e-3,\n",
    "    latency_type='mmac',\n",
    "    scheduler=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get regular model with the best architecture\n",
    "best_model = search_space.get_network_with_best_arch().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = RAdam(best_model.parameters(), lr=5e-3, weight_decay=4e-5)\n",
    "\n",
    "tutorial_train_loop(\n",
    "    epochs=5,\n",
    "    model=best_model,\n",
    "    optimizer=optimizer,\n",
    "    metric_function=accuracy,\n",
    "    loss_function=loss_function,\n",
    "    train_loader=dataloaders['tune_train_dataloader'],\n",
    "    validation_loader=dataloaders['tune_validation_dataloader'],\n",
    "    scheduler=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
