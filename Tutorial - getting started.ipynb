{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with ENOT\n",
    "\n",
    "This notebook describes the basic steps you need to optimize an architecture.\n",
    "\n",
    "### Notebook consists of next main stages:\n",
    "1. Setup the environment\n",
    "1. Prepare dataset and create dataloaders\n",
    "1. Create the model and move it to search space\n",
    "1. Pretrain constructed search space\n",
    "1. Search best architecture\n",
    "1. Tune model with best architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup the environment\n",
    "First, let's set up the environment and common imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy your license file to $HOME/.enot/enot.lic or set full path to licence file \n",
    "# through environment variable ENOT_LIC_FILE\n",
    "#\n",
    "# Important note: no quotes\n",
    "# %env ENOT_LIC_FILE=/FULL/PATH/TO/your_company.lic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "# You should change to free GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch_optimizer import RAdam\n",
    "\n",
    "from enot.models import SearchSpaceModel\n",
    "from enot.models.mobilenet import build_mobilenet\n",
    "from enot.phases import pretrain\n",
    "from enot.phases import search\n",
    "from enot.phases import train\n",
    "\n",
    "from enot_utils.metric_utils import accuracy\n",
    "from enot_utils.schedulers import WarmupScheduler\n",
    "\n",
    "from tutorial_utils.checkpoints import download_getting_started_pretrain_checkpoint\n",
    "from tutorial_utils.dataset import create_imagenette_dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the next cell we setup all required dirs\n",
    "\n",
    "* `ENOT_HOME_DIR` - is root dir for all other dirs\n",
    "* `ENOT_DATASETS_DIR` - is root dir for datasets (imagenette2)\n",
    "* `PROJECT_DIR` - is root dir for output data (checkpoints, logs...) of current tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENOT_HOME_DIR = Path.home() / '.enot'\n",
    "ENOT_DATASETS_DIR = ENOT_HOME_DIR / 'datasets'\n",
    "PROJECT_DIR = ENOT_HOME_DIR / 'getting_started'\n",
    "\n",
    "ENOT_HOME_DIR.mkdir(exist_ok=True)\n",
    "ENOT_DATASETS_DIR.mkdir(exist_ok=True)\n",
    "PROJECT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset and create dataloaders\n",
    "\n",
    "In this example we will use dataset Imagenette2. <br>\n",
    "`create_imagenette_dataloaders` function do all dirty work: \n",
    "1. automatically downloads and unpack dataset in `ENOT_DATASETS_DIR`\n",
    "1. split dataset on 4 parts and save annotation of every part in `PROJECT_DIR`:\n",
    "    * train - for pretrain and tuning stages (`PROJECT_DIR`/train.csv)\n",
    "    * validation - to choose checkpoint for architecture optimization (`PROJECT_DIR`/validation.csv)\n",
    "    * search - for architecture optimization stage (`PROJECT_DIR`/search.csv)\n",
    "    * test - held-out data for testing (`PROJECT_DIR`/test.csv)\n",
    "1. create dataloaders for every stage and returns dataloaders as dictionary:\n",
    "    * pretrain_train_dataloader - pretrain train dataloader (based on `PROJECT_DIR`/train.csv)\n",
    "    * pretrain_validation_dataloader - pretrain validation dataloader (based on `PROJECT_DIR`/validation.csv)\n",
    "    * search_train_dataloader - search train dataloader (based on `PROJECT_DIR`/search.csv)\n",
    "    * search_validation_dataloader - search validation dataloader (based on `PROJECT_DIR`/search.csv)\n",
    "    * tune_train_dataloader - tune train dataloader (the same as pretrain_train_dataloader)\n",
    "    * tune_validation_dataloader - tune validation dataloader (the same as pretrain_validation_dataloader)\n",
    "\n",
    "**NOTE:**<br>\n",
    "CSV annotations follow this format:\n",
    "```\n",
    "filepath,label\n",
    "<relative_path_1>,<int_label_1>\n",
    "<relative_path_2>,<int_label_2>\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = create_imagenette_dataloaders(\n",
    "    dataset_root_dir=ENOT_DATASETS_DIR, \n",
    "    project_dir=PROJECT_DIR,\n",
    "    input_size=(224, 224),\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the model and move it to search space\n",
    "\n",
    "Our architecture optimization procedure selects the best combination of operations from a user-defined search space. The easiest way to define one is to take a base architecture and add similar operations with different parameters: kernel size, expansion ratio, etc. You can also add different kinds of operations or implement your own (see \"Tutorial - adding a custom operation\" for this).\n",
    "\n",
    "In this example we take MobileNet v2 as a base architecture and make a search space from MobileNet inverted bottleneck blocks.\n",
    "\n",
    "First, let's define a model for image classification tasks. MobileNet like models can be built by `build_mobilenet` function. `build_mobilenet` function returns model with the following structure: (inputs) -> stem -> blocks -> head -> (outputs). Stem and head have a fixed structure, blocks consist of user-defined operations to choose from at the search stage. The template class will follow MobileNet v2 structure, [as seen in torchvision](https://github.com/pytorch/vision/blob/master/torchvision/models/mobilenet.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The search space will have these ops to choose from in each layer\n",
    "# Short format for operations is 'Name_param1=value1_param2=value2...'\n",
    "# MIB is MNv2 inverted bottleneck. k is kernel size, t is expansion ratio\n",
    "# See more in-depth info in \"Tutorial - adding custom operations\"\n",
    "SEARCH_OPS = [\n",
    "    'MIB_k=3_t=6',\n",
    "    'MIB_k=5_t=6',\n",
    "    'MIB_k=7_t=6',\n",
    "]\n",
    "\n",
    "# build model\n",
    "model = build_mobilenet(\n",
    "    search_ops=SEARCH_OPS,\n",
    "    num_classes=10,\n",
    "    blocks_out_channels=[24, 32, 64, 96, 160, 320],\n",
    "    blocks_count=[2, 2, 2, 1, 2, 1],\n",
    "    blocks_stride=[2, 2, 2, 1, 2, 1],\n",
    ")\n",
    "# move model to search space\n",
    "search_space = SearchSpaceModel(model, train_loader=dataloaders['pretrain_train_dataloader']).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pretrain constructed search space\n",
    "\n",
    "We've implemented a default training procedure for you. You only need to set up search space, data loaders, transforms, and optimizer and run it.\n",
    "\n",
    "**IMPORTANT:**<br>\n",
    "`N_EPOCHS` should be in range >= 300, if you wanna get good pretrain. In this tutorial we set `N_EPOCHS` = 3 and download checkpoint of trained model from Google Drive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define directory for text logs and tensorboard logs\n",
    "pretrain_dir = PROJECT_DIR / 'pretrain'\n",
    "pretrain_dir.mkdir(exist_ok=True)\n",
    "\n",
    "N_EPOCHS = 1\n",
    "N_WARMUP_EPOCHS = 1\n",
    "len_train = len(dataloaders['pretrain_train_dataloader'])\n",
    "\n",
    "optimizer = SGD(params=search_space.model_parameters(), lr=0.06, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=len_train*N_EPOCHS, eta_min=1e-8)\n",
    "scheduler = WarmupScheduler(scheduler, warmup_steps=len_train*N_WARMUP_EPOCHS)\n",
    "loss_function = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "pretrain(\n",
    "    search_space=search_space,\n",
    "    exp_dir=pretrain_dir,\n",
    "    train_loader=dataloaders['pretrain_train_dataloader'],\n",
    "    valid_loader=dataloaders['pretrain_validation_dataloader'],\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    metric_function=accuracy,\n",
    "    loss_function=loss_function,\n",
    "    epochs=N_EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You can find best pretrain checkpoint and load it into your search space.\n",
    "checkpoint_path = pretrain_dir / 'getting_started_pretrain_checkpoint.pth'\n",
    "download_getting_started_pretrain_checkpoint(checkpoint_path)\n",
    "\n",
    "search_space.load_state_dict(\n",
    "    torch.load(checkpoint_path)['model'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Search best architecture\n",
    "Now that you have a trained search space you can run the search phase. The setup is similar to pretrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define directory for text logs and tensorboard logs\n",
    "search_dir = PROJECT_DIR / 'search'\n",
    "search_dir.mkdir(exist_ok=True)\n",
    "\n",
    "optimizer = RAdam(search_space.architecture_parameters(), lr=0.01)\n",
    "\n",
    "search(\n",
    "    search_space=search_space,\n",
    "    exp_dir=search_dir,\n",
    "    search_loader=dataloaders['search_train_dataloader'],\n",
    "    valid_loader=dataloaders['search_validation_dataloader'],\n",
    "    optimizer=optimizer,\n",
    "    loss_function=loss_function,\n",
    "    metric_function=accuracy,\n",
    "    latency_loss_weight=2.0e-3,\n",
    "    epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tune model with best architecture\n",
    "Now we take the best architecture in search space and create a regular model using it, then we run finetune procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get regular model with best architecture\n",
    "best_model = search_space.get_network_with_best_arch().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define directory for text logs and tensorboard logs\n",
    "tune_dir = PROJECT_DIR / 'tune'\n",
    "tune_dir.mkdir(exist_ok=True)\n",
    "\n",
    "optimizer = RAdam(best_model.parameters(), lr=5e-3, weight_decay=4e-5)\n",
    "\n",
    "train(\n",
    "    model=best_model,\n",
    "    exp_dir=tune_dir,\n",
    "    train_loader=dataloaders['tune_train_dataloader'],\n",
    "    valid_loader=dataloaders['tune_validation_dataloader'],\n",
    "    optimizer=optimizer,\n",
    "    loss_function=loss_function,\n",
    "    metric_function=accuracy,\n",
    "    epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enot_venv",
   "language": "python",
   "name": ".enot_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
