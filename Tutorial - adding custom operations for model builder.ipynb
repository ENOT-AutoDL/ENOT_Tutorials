{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding custom operations for model builder\n",
    "\n",
    "This notebook describes ways to implement your own operations, and how to use them with model builder.\n",
    "\n",
    "### Main chapters of this notebook:\n",
    "1. Setup the environment\n",
    "1. Add a custom operation for model builder\n",
    "1. Prepare dataset and create dataloaders\n",
    "1. Build model with custom operations\n",
    "1. Check pretrain, search and tune phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the environment\n",
    "\n",
    "First, let's set up the environment and make some common imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "# You may need to change this variable to match free GPU index\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch_optimizer import RAdam\n",
    "\n",
    "from enot.models import register_searchable_op\n",
    "from enot.models import SearchSpaceModel\n",
    "from enot.models.mobilenet import build_mobilenet\n",
    "from enot.optimize import EnotPretrainOptimizer\n",
    "from enot.optimize import EnotSearchOptimizer\n",
    "\n",
    "from tutorial_utils.train_utils import accuracy\n",
    "from tutorial_utils.train_utils import WarmupScheduler\n",
    "\n",
    "from tutorial_utils.dataset import create_imagenette_dataloaders\n",
    "from tutorial_utils.phases import tutorial_pretrain_loop\n",
    "from tutorial_utils.phases import tutorial_search_loop\n",
    "from tutorial_utils.phases import tutorial_train_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add custom operation for model builder\n",
    "\n",
    "To create custom operation, and use it with our model builders (`build_mobilenet`), follow these rules:\n",
    "1. register operation in our framework using `@register_searchable_op(name)`;\n",
    "1. it should accept 4 positional arguments: `in_channels`, `out_channels`, `stride`, `use_skip_connection`. All other arguments should have default value, or must be added in short config (see the next paragraph);\n",
    "1. if you want to use the short config format, you must set parameter parsing rules;\n",
    "1. if you need to search architecture w.r.t its latency, you must add `LatencyMixin` to you custom operation (see \"Tutorial - using latency optimization\").\n",
    "\n",
    "#### Example of parameter parsing rules:\n",
    "\n",
    "If you want to write `MyOp_k=3_a=relu` instead of `{'op_type': 'MyOp', 'kernel_size': 3, 'activation': 'relu'}`, you need the following rules:\n",
    "```\n",
    "{\n",
    "  'k': ('kernel_size', int),\n",
    "  'a': ('activation', str),\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {\n",
    "    'relu': nn.ReLU,\n",
    "    'sigmoid': nn.Sigmoid,\n",
    "}\n",
    "\n",
    "# defineing short parameter parsing rules\n",
    "# format: {short_param_name: (original_param_name, parser)}\n",
    "short_args = {\n",
    "    'k': ('kernel_size', int),\n",
    "    'a': ('activation', lambda x: activations[x])\n",
    "}\n",
    "\n",
    "\n",
    "@register_searchable_op('MyOp', short_args)\n",
    "class MyOperation(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        stride,\n",
    "        kernel_size=3,\n",
    "        activation=nn.ReLU,\n",
    "        padding=None,   \n",
    "        use_skip_connection=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_skip_connection = use_skip_connection \n",
    "\n",
    "        if padding is None:\n",
    "            padding = (kernel_size - 1) // 2\n",
    "\n",
    "        self.operation = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels, \n",
    "                out_channels=out_channels, \n",
    "                kernel_size=kernel_size, \n",
    "                stride=stride, \n",
    "                padding=padding,\n",
    "            ),\n",
    "            activation(),  # for example, you can use activation before batch normalization layer\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.operation(x)\n",
    "\n",
    "        if self.use_skip_connection and x.shape == y.shape:\n",
    "            y += x\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset and create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENOT_HOME_DIR = Path.home() / '.enot'\n",
    "ENOT_DATASETS_DIR = ENOT_HOME_DIR / 'datasets'\n",
    "PROJECT_DIR = ENOT_HOME_DIR / 'custom_operation'\n",
    "\n",
    "ENOT_HOME_DIR.mkdir(exist_ok=True)\n",
    "ENOT_DATASETS_DIR.mkdir(exist_ok=True)\n",
    "PROJECT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "dataloaders = create_imagenette_dataloaders(\n",
    "    dataset_root_dir=ENOT_DATASETS_DIR, \n",
    "    project_dir=PROJECT_DIR,\n",
    "    input_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    imagenette_kind='imagenette2-320',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model with custom operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_OPS = [\n",
    "    'MIB_k=3_t=6',\n",
    "    'MyOp_k=3',  # Notice that you can omit parameters with default values\n",
    "    'MyOp_k=3_a=sigmoid',\n",
    "]\n",
    "\n",
    "# build model\n",
    "model = build_mobilenet(\n",
    "    search_ops=SEARCH_OPS,\n",
    "    num_classes=10,\n",
    "    blocks_out_channels=[24, 32, 64, 96, 160, 320],\n",
    "    blocks_count=[2, 2, 2, 1, 2, 1],\n",
    "    blocks_stride=[2, 2, 2, 1, 2, 1],\n",
    ")\n",
    "\n",
    "# move model to search space\n",
    "search_space = SearchSpaceModel(model).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check pretrain search and tune phases\n",
    "\n",
    "Let's check that everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 3\n",
    "N_WARMUP_EPOCHS = 1\n",
    "len_train = len(dataloaders['pretrain_train_dataloader'])\n",
    "\n",
    "optimizer = SGD(params=search_space.model_parameters(), lr=0.06, momentum=0.9, weight_decay=1e-4)\n",
    "enot_optimizer = EnotPretrainOptimizer(search_space=search_space, optimizer=optimizer)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=len_train*N_EPOCHS, eta_min=1e-8)\n",
    "scheduler = WarmupScheduler(scheduler, warmup_steps=len_train*N_WARMUP_EPOCHS)\n",
    "loss_function = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "tutorial_pretrain_loop(\n",
    "    epochs=N_EPOCHS,\n",
    "    search_space=search_space,\n",
    "    enot_optimizer=enot_optimizer,\n",
    "    metric_function=accuracy,\n",
    "    loss_function=loss_function,\n",
    "    train_loader=dataloaders['pretrain_train_dataloader'],\n",
    "    validation_loader=dataloaders['pretrain_validation_dataloader'],\n",
    "    scheduler=scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RAdam(search_space.architecture_parameters(), lr=0.01)\n",
    "enot_optimizer = EnotSearchOptimizer(search_space=search_space, optimizer=optimizer)\n",
    "\n",
    "tutorial_search_loop(\n",
    "    epochs=5,\n",
    "    search_space=search_space,\n",
    "    enot_optimizer=enot_optimizer,\n",
    "    metric_function=accuracy,\n",
    "    loss_function=loss_function,\n",
    "    train_loader=dataloaders['search_train_dataloader'],\n",
    "    validation_loader=dataloaders['search_validation_dataloader'],\n",
    "    latency_loss_weight=0,\n",
    "    latency_type=None,\n",
    "    scheduler=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get regular model with the best architecture\n",
    "best_model = search_space.get_network_with_best_arch().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RAdam(best_model.parameters(), lr=5e-3, weight_decay=4e-5)\n",
    "\n",
    "tutorial_train_loop(\n",
    "    epochs=5,\n",
    "    model=best_model,\n",
    "    optimizer=optimizer,\n",
    "    metric_function=accuracy,\n",
    "    loss_function=loss_function,\n",
    "    train_loader=dataloaders['tune_train_dataloader'],\n",
    "    validation_loader=dataloaders['tune_validation_dataloader'],\n",
    "    scheduler=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
