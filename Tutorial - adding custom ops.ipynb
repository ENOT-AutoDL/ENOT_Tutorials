{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding custom operations to ENOT\n",
    "\n",
    "In this notebook we describe how you can implement your own operations to use with neural architecture search.\n",
    "\n",
    "### Notebook consists of next main stages:\n",
    "1. Setup the environment\n",
    "1. Add a custom operation to use with search space\n",
    "1. Build model with custom operation\n",
    "1. Check pretrain search and tune phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup the environment\n",
    "First, let's set up the environment and common imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "# You should change to free GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch_optimizer import RAdam\n",
    "\n",
    "from enot.models import BaseSearchableOperation\n",
    "from enot.models import register_searchable_op\n",
    "from enot.models import SearchSpaceModel\n",
    "from enot.models.mobilenet import build_mobilenet\n",
    "from enot.phases import pretrain\n",
    "from enot.phases import search\n",
    "from enot.phases import train\n",
    "\n",
    "from enot_utils.metric_utils import accuracy\n",
    "from enot_utils.schedulers import WarmupScheduler\n",
    "\n",
    "from tutorial_utils.dataset import create_imagenette_dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add a custom operation to use with search space\n",
    "\n",
    "We have provided a bunch of pre-defined operations, but you can also implement your own.\n",
    "\n",
    "You must follow next steps to create new searchable operation:\n",
    "1. Create new operation with base class `BaseSearchableOperation`\n",
    "    * The last layer of your operation should be Batch Normalization\n",
    "1. Initialize base class in you `__init__`. `BaseSearchableOperation` operation takes 4 required arguments:\n",
    "    * in_channels - number of channels in the input data\n",
    "    * out_channels - number of channels produced by the operation\n",
    "    * use_skip_connection - if you wanna apply skip connection set `True`, and `False` otherwise\n",
    "1. Implement abstract method `get_last_batch_norm`, which returns last Batch Normalization layer\n",
    "1. Implement abstract method `replace_last_batch_norm`, which replace last Batch Normalization layer by new one\n",
    "1. Implement abstract method `operation_forward`, which defines the computation performed at every call like common \"forward\", but ignore skip connection logic (skip connection logic implemented in \"forward\" method of base class) \n",
    "\n",
    "If you wanna use custom operation with our model builders (`build_mobilenet`):\n",
    "1. They must be registered in our framework using `@register_searchable_op(name)`\n",
    "1. Operation must accept 4 required argument: in_channels, out_channels, stride, use_skip_connection. All other arguments must have default value or must be added in short config (see next paragraph).\n",
    "1. To use the short config format you must provide parameter parsing rules E. g. if you want to write `MyOp_k=3_t=6` instead of `{\"op_type\": \"MyOp\", \"kernel_size\": 3, \"expand_ratio\": 6.0}`, you need the following rules:\n",
    "```\n",
    "    {\n",
    "      'k': ('kernel_size', int),\n",
    "      't': ('expand_ratio', float)\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {\n",
    "    'relu': nn.ReLU,\n",
    "    'sigmoid': nn.Sigmoid\n",
    "}\n",
    "# Define short parameter parsing rules\n",
    "# Format: {short_param_name: (original_param_name, parser)}\n",
    "short_args = {\n",
    "    'k': ('kernel_size', int),\n",
    "    'a': ('activation', lambda x: activations[x])\n",
    "}\n",
    "\n",
    "\n",
    "@register_searchable_op('MyOp', short_args)\n",
    "class MyOperation(BaseSearchableOperation):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        stride,\n",
    "        kernel_size=3,\n",
    "        activation=nn.ReLU,\n",
    "        padding=None,   \n",
    "        use_skip_connection=True\n",
    "    ):\n",
    "        super().__init__(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            use_skip_connection,\n",
    "        )\n",
    "            \n",
    "        if padding is None:\n",
    "            padding = (kernel_size - 1) // 2\n",
    "        \n",
    "        self.operation = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels, \n",
    "                out_channels=out_channels, \n",
    "                kernel_size=kernel_size, \n",
    "                stride=stride, \n",
    "                padding=padding,\n",
    "            ),\n",
    "            activation(),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def get_last_batch_norm(self) -> nn.BatchNorm2d:\n",
    "        return self.operation[-1]\n",
    "\n",
    "    def replace_last_batch_norm(self, new_last_batch_norm: nn.BatchNorm2d) -> None:\n",
    "        self.operation[-1] = new_last_batch_norm\n",
    "        \n",
    "    def operation_forward(self, x):\n",
    "        return self.operation(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build model with custom operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_OPS = [\n",
    "    'MIB_k=3_t=6',\n",
    "    'MIB_k=5_t=6',\n",
    "    'MIB_k=7_t=6',\n",
    "    'MyOp_k=3',  # Notice that you can omit parameters with default values\n",
    "    'MyOp_k=3_a=sigmoid',\n",
    "]\n",
    "\n",
    "# build model\n",
    "model = build_mobilenet(\n",
    "    search_ops=SEARCH_OPS,\n",
    "    num_classes=10,\n",
    "    blocks_out_channels=[24, 32, 64, 96, 160, 320],\n",
    "    blocks_count=[2, 2, 2, 1, 2, 1],\n",
    "    blocks_stride=[2, 2, 2, 1, 2, 1],\n",
    ")\n",
    "# move model to search space\n",
    "search_space = SearchSpaceModel(model).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check pretrain search and tune phases\n",
    "\n",
    "Let's check that everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENOT_HOME_DIR = Path.home() / '.enot'\n",
    "ENOT_DATASETS_DIR = ENOT_HOME_DIR / 'datasets'\n",
    "PROJECT_DIR = ENOT_HOME_DIR / 'custom_operation'\n",
    "\n",
    "ENOT_HOME_DIR.mkdir(exist_ok=True)\n",
    "ENOT_DATASETS_DIR.mkdir(exist_ok=True)\n",
    "PROJECT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = create_imagenette_dataloaders(\n",
    "    dataset_root_dir=ENOT_DATASETS_DIR, \n",
    "    project_dir=PROJECT_DIR,\n",
    "    input_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    imagenette_kind='imagenette2-320',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define directory for text logs and tensorboard logs\n",
    "pretrain_dir = PROJECT_DIR / 'pretrain'\n",
    "pretrain_dir.mkdir(exist_ok=True)\n",
    "\n",
    "N_EPOCHS = 3\n",
    "N_WARMUP_EPOCHS = 1\n",
    "len_train = len(dataloaders['pretrain_train_dataloader'])\n",
    "\n",
    "optimizer = SGD(params=search_space.model_parameters(), lr=0.06, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=len_train*N_EPOCHS, eta_min=1e-8)\n",
    "scheduler = WarmupScheduler(scheduler, warmup_steps=len_train*N_WARMUP_EPOCHS)\n",
    "loss_function = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "pretrain(\n",
    "    search_space=search_space,\n",
    "    exp_dir=pretrain_dir,\n",
    "    train_loader=dataloaders['pretrain_train_dataloader'],\n",
    "    valid_loader=dataloaders['pretrain_validation_dataloader'],\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    metric_function=accuracy,\n",
    "    loss_function=loss_function,\n",
    "    epochs=N_EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directory for text logs and tensorboard logs\n",
    "search_dir = PROJECT_DIR / 'search'\n",
    "search_dir.mkdir(exist_ok=True)\n",
    "\n",
    "optimizer = RAdam(search_space.architecture_parameters(), lr=0.01)\n",
    "\n",
    "search(\n",
    "    search_space=search_space,\n",
    "    exp_dir=search_dir,\n",
    "    search_loader=dataloaders['search_train_dataloader'],\n",
    "    valid_loader=dataloaders['search_validation_dataloader'],\n",
    "    optimizer=optimizer,\n",
    "    loss_function=loss_function,\n",
    "    metric_function=accuracy,\n",
    "    epochs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get regular model with best architecture\n",
    "best_model = search_space.get_network_with_best_arch().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directory for text logs and tensorboard logs\n",
    "tune_dir = PROJECT_DIR / 'tune'\n",
    "tune_dir.mkdir(exist_ok=True)\n",
    "\n",
    "optimizer = RAdam(best_model.parameters(), lr=5e-3, weight_decay=4e-5)\n",
    "\n",
    "train(\n",
    "    model=best_model,\n",
    "    exp_dir=tune_dir,\n",
    "    train_loader=dataloaders['tune_train_dataloader'],\n",
    "    valid_loader=dataloaders['tune_validation_dataloader'],\n",
    "    optimizer=optimizer,\n",
    "    loss_function=loss_function,\n",
    "    metric_function=accuracy,\n",
    "    epochs=3,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
