{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENOT baseline optimizer\n",
    "\n",
    "This notebook describes how to use ENOT optimizer.\n",
    "\n",
    "### Main chapters of this notebook:\n",
    "1. Setup the environment\n",
    "1. Prepare dataset and create dataloaders\n",
    "1. Train model using ENOT baseline optimizer\n",
    "1. Check ENOT optimizer profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENOT optimizer wraps any PyTorch optimizer and can increase metrics.\n",
    "It was tested on many computer vision classification tasks, other tasks are not tested.\n",
    "Before trying our optimizer we recommend tuning your training hyperparameters (optimizer, scheduler, learning rate etc). Our optimizer is twice as slow as the original PyTorch optimizer.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the environment\n",
    "First, let's set up the environment and make some common imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "# You may need to uncomment and change this variable to match free GPU index\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch_optimizer import RAdam\n",
    "from torchvision.models.mobilenet import mobilenet_v2\n",
    "\n",
    "from enot.optimize import GTBaselineOptimizer\n",
    "\n",
    "from tutorial_utils.train import accuracy\n",
    "from tutorial_utils.train import WarmupScheduler\n",
    "\n",
    "from tutorial_utils.dataset import create_imagenette_dataloaders\n",
    "from tutorial_utils.phases import tutorial_train_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the following cell we setup all necessary dirs\n",
    "\n",
    "* `HOME_DIR` - experiments home directory\n",
    "* `DATASETS_DIR` - root directory for datasets (imagenette2, ...)\n",
    "* `PROJECT_DIR` - project directory to save training logs, checkpoints, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = Path.home() / '.optimization_experiments'\n",
    "DATASETS_DIR = HOME_DIR / 'datasets'\n",
    "PROJECT_DIR = HOME_DIR / 'search_space_autogeneration'\n",
    "\n",
    "HOME_DIR.mkdir(exist_ok=True)\n",
    "DATASETS_DIR.mkdir(exist_ok=True)\n",
    "PROJECT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset and create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = create_imagenette_dataloaders(\n",
    "    dataset_root_dir=DATASETS_DIR,\n",
    "    project_dir=PROJECT_DIR,\n",
    "    input_size=(224, 224),\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model with ENOT Baseline optimizer\n",
    "\n",
    "**IMPORTANT:**<br>\n",
    "We set `N_EPOCHS`= 3 in this example to make tutorial execution faster. This is not enough for good train quality, and you should set `N_EPOCHS`>= 100 if you want to achieve good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use our optimizer wrap the model train step with a closure (closure should clear gradients, compute loss and return it), and pass the closure into `enot_optimizer.step(...)` method as argument. It is necessary because `GTBaselineOptimizer` does more than one step per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mobilenet_v2(weights=None)\n",
    "\n",
    "classifier = model.classifier[1]\n",
    "model.classifier = nn.Linear(\n",
    "    in_features=classifier.in_features,\n",
    "    out_features=10,\n",
    "    bias=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 3\n",
    "N_WARMUP_EPOCHS = 1\n",
    "\n",
    "len_train = len(dataloaders['tune_train_dataloader'])\n",
    "\n",
    "optimizer = SGD(params=model.parameters(), lr=0.06, momentum=0.9, weight_decay=1e-4)\n",
    "enot_optimizer = GTBaselineOptimizer(model=model, optimizer=optimizer)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=len_train * N_EPOCHS, eta_min=1e-8)\n",
    "# you can also use enot_optimizer._optimizer\n",
    "scheduler = WarmupScheduler(scheduler, warmup_steps=len_train * N_WARMUP_EPOCHS)\n",
    "loss_function = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "train_loader = dataloaders['tune_train_dataloader']\n",
    "validation_loader = dataloaders['tune_validation_dataloader']\n",
    "\n",
    "model.cuda()\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f'EPOCH #{epoch}')\n",
    "\n",
    "    model.train()\n",
    "    train_metrics_accumulator = {\n",
    "        'loss': 0.0,\n",
    "        'accuracy': 0.0,\n",
    "        'n': 0,\n",
    "    }\n",
    "    for inputs, labels in train_loader:\n",
    "        enot_optimizer.zero_grad()\n",
    "\n",
    "        def closure():\n",
    "            pred_labels = model(inputs)\n",
    "            batch_loss = loss_function(pred_labels, labels)\n",
    "            batch_loss.backward()\n",
    "            batch_metric = accuracy(pred_labels, labels)\n",
    "\n",
    "            train_metrics_accumulator['loss'] += batch_loss.item()\n",
    "            train_metrics_accumulator['accuracy'] += batch_metric.item()\n",
    "            train_metrics_accumulator['n'] += 1\n",
    "            return batch_loss\n",
    "\n",
    "        enot_optimizer.step(closure)\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    train_loss = train_metrics_accumulator['loss'] / train_metrics_accumulator['n']\n",
    "    train_accuracy = train_metrics_accumulator['accuracy'] / train_metrics_accumulator['n']\n",
    "\n",
    "    print('train metrics:')\n",
    "    print('  loss:', train_loss)\n",
    "    print('  accuracy:', train_accuracy)\n",
    "\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    validation_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validation_loader:\n",
    "            pred_labels = model(inputs)\n",
    "            batch_loss = loss_function(pred_labels, labels)\n",
    "            batch_metric = accuracy(pred_labels, labels)\n",
    "\n",
    "            validation_loss += batch_loss.item()\n",
    "            validation_accuracy += batch_metric.item()\n",
    "\n",
    "    n = len(validation_loader)\n",
    "    validation_loss /= n\n",
    "    validation_accuracy /= n\n",
    "\n",
    "    print('validation metrics:')\n",
    "    print('  loss:', validation_loss)\n",
    "    print('  accuracy:', validation_accuracy)\n",
    "\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 3000
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
