{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding custom operations to ENOT\n",
    "\n",
    "In this notebook we describe how you can implement your own operations to use with neural architecture search.\n",
    "\n",
    "### Notebook consists of next main stages:\n",
    "1. Setup the environment\n",
    "1. Add a custom operation to use with search space\n",
    "1. Build model with custom operation\n",
    "1. Check pretrain search and tune phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup the environment\n",
    "First, let's set up the environment and common imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy your license file to $HOME/.enot/enot.lic or set full path to licence file \n",
    "# through environment variable ENOT_LIC_FILE\n",
    "#\n",
    "# Important note: no quotes\n",
    "# %env ENOT_LIC_FILE=/FULL/PATH/TO/your_company.lic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "# You should change to free GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch_optimizer import RAdam\n",
    "\n",
    "from enot.models import register_searchable_op\n",
    "from enot.models import SearchSpaceModel\n",
    "from enot.models.mobilenet import build_mobilenet\n",
    "from enot.phases import pretrain\n",
    "from enot.phases import search\n",
    "from enot.phases import train\n",
    "\n",
    "from enot_utils.metric_utils import accuracy\n",
    "from enot_utils.schedulers import WarmupScheduler\n",
    "\n",
    "from tutorial_utils.dataset import create_imagenette_dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add a custom operation to use with search space\n",
    "\n",
    "We have provided a bunch of pre-defined operations, but you can also implement your own as you have done it in PyTorch. For better performance of out frame work there are two simple recommendations for op creation:\n",
    "1. Last layer in your op should be BatchNormalization layer with affine transformation.\n",
    "2. All search branches should have the same opeations and their application sequence.   \n",
    "\n",
    "If you wanna use custom operation with our model builders (`build_mobilenet`):\n",
    "1. They must be registered in our framework using `@register_searchable_op(name)`\n",
    "1. Operation must accept 4 required argument: in_channels, out_channels, stride, use_skip_connection. All other arguments must have default value or must be added in short config (see next paragraph).\n",
    "1. To use the short config format you must provide parameter parsing rules E. g. if you want to write `MyOp_k=3_t=6` instead of `{\"op_type\": \"MyOp\", \"kernel_size\": 3, \"expand_ratio\": 6.0}`, you need the following rules:\n",
    "```\n",
    "    {\n",
    "      'k': ('kernel_size', int),\n",
    "      't': ('expand_ratio', float)\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {\n",
    "    'relu': nn.ReLU,\n",
    "    'sigmoid': nn.Sigmoid\n",
    "}\n",
    "# Define short parameter parsing rules\n",
    "# Format: {short_param_name: (original_param_name, parser)}\n",
    "short_args = {\n",
    "    'k': ('kernel_size', int),\n",
    "    'a': ('activation', lambda x: activations[x])\n",
    "}\n",
    "\n",
    "\n",
    "@register_searchable_op('MyOp', short_args)\n",
    "class MyOperation(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        stride,\n",
    "        kernel_size=3,\n",
    "        activation=nn.ReLU,\n",
    "        padding=None,   \n",
    "        use_skip_connection=True\n",
    "    ):\n",
    "        self.use_skip_connection = use_skip_connection \n",
    "        super().__init__()\n",
    "            \n",
    "        if padding is None:\n",
    "            padding = (kernel_size - 1) // 2\n",
    "        \n",
    "        self.operation = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels, \n",
    "                out_channels=out_channels, \n",
    "                kernel_size=kernel_size, \n",
    "                stride=stride, \n",
    "                padding=padding,\n",
    "            ),\n",
    "            activation(),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.operation(x)\n",
    "        \n",
    "        if self.use_skip_connection and x.shape == y.shape:\n",
    "            y += x\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENOT_HOME_DIR = Path.home() / '.enot'\n",
    "ENOT_DATASETS_DIR = ENOT_HOME_DIR / 'datasets'\n",
    "PROJECT_DIR = ENOT_HOME_DIR / 'custom_operation'\n",
    "\n",
    "ENOT_HOME_DIR.mkdir(exist_ok=True)\n",
    "ENOT_DATASETS_DIR.mkdir(exist_ok=True)\n",
    "PROJECT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "dataloaders = create_imagenette_dataloaders(\n",
    "    dataset_root_dir=ENOT_DATASETS_DIR, \n",
    "    project_dir=PROJECT_DIR,\n",
    "    input_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    imagenette_kind='imagenette2-320',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build model with custom operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_OPS = [\n",
    "    'MIB_k=3_t=6',\n",
    "    'MIB_k=5_t=6',\n",
    "    'MIB_k=7_t=6',\n",
    "    'MyOp_k=3',  # Notice that you can omit parameters with default values\n",
    "    'MyOp_k=3_a=sigmoid',\n",
    "]\n",
    "\n",
    "# build model\n",
    "model = build_mobilenet(\n",
    "    search_ops=SEARCH_OPS,\n",
    "    num_classes=10,\n",
    "    blocks_out_channels=[24, 32, 64, 96, 160, 320],\n",
    "    blocks_count=[2, 2, 2, 1, 2, 1],\n",
    "    blocks_stride=[2, 2, 2, 1, 2, 1],\n",
    ")\n",
    "# move model to search space\n",
    "search_space = SearchSpaceModel(model, train_loader=dataloaders['pretrain_train_dataloader']).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check pretrain search and tune phases\n",
    "\n",
    "Let's check that everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define directory for text logs and tensorboard logs\n",
    "pretrain_dir = PROJECT_DIR / 'pretrain'\n",
    "pretrain_dir.mkdir(exist_ok=True)\n",
    "\n",
    "N_EPOCHS = 3\n",
    "N_WARMUP_EPOCHS = 1\n",
    "len_train = len(dataloaders['pretrain_train_dataloader'])\n",
    "\n",
    "optimizer = SGD(params=search_space.model_parameters(), lr=0.06, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=len_train*N_EPOCHS, eta_min=1e-8)\n",
    "scheduler = WarmupScheduler(scheduler, warmup_steps=len_train*N_WARMUP_EPOCHS)\n",
    "loss_function = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "pretrain(\n",
    "    search_space=search_space,\n",
    "    exp_dir=pretrain_dir,\n",
    "    train_loader=dataloaders['pretrain_train_dataloader'],\n",
    "    valid_loader=dataloaders['pretrain_validation_dataloader'],\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    metric_function=accuracy,\n",
    "    loss_function=loss_function,\n",
    "    epochs=N_EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directory for text logs and tensorboard logs\n",
    "search_dir = PROJECT_DIR / 'search'\n",
    "search_dir.mkdir(exist_ok=True)\n",
    "\n",
    "optimizer = RAdam(search_space.architecture_parameters(), lr=0.01)\n",
    "\n",
    "search(\n",
    "    search_space=search_space,\n",
    "    exp_dir=search_dir,\n",
    "    search_loader=dataloaders['search_train_dataloader'],\n",
    "    valid_loader=dataloaders['search_validation_dataloader'],\n",
    "    optimizer=optimizer,\n",
    "    loss_function=loss_function,\n",
    "    metric_function=accuracy,\n",
    "    epochs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get regular model with best architecture\n",
    "best_model = search_space.get_network_with_best_arch().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directory for text logs and tensorboard logs\n",
    "tune_dir = PROJECT_DIR / 'tune'\n",
    "tune_dir.mkdir(exist_ok=True)\n",
    "\n",
    "optimizer = RAdam(best_model.parameters(), lr=5e-3, weight_decay=4e-5)\n",
    "\n",
    "train(\n",
    "    model=best_model,\n",
    "    exp_dir=tune_dir,\n",
    "    train_loader=dataloaders['tune_train_dataloader'],\n",
    "    valid_loader=dataloaders['tune_validation_dataloader'],\n",
    "    optimizer=optimizer,\n",
    "    loss_function=loss_function,\n",
    "    metric_function=accuracy,\n",
    "    epochs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enot_venv",
   "language": "python",
   "name": ".enot_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
