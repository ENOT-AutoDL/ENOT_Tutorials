{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic pruning\n",
    "\n",
    "This notebook demonstrates end2end pipeline for MobileNetV2 pruning.\n",
    "\n",
    "This notebook assumes that you want to integrate pruning into your training script and modify it for fine-tuning. \n",
    "\n",
    "Our pruning process consists of calibration for pruning, least important channel selection, channel pruning and model fine-tuning.\n",
    "\n",
    "### Main chapters of this notebook:\n",
    "1. Setup the environment\n",
    "1. Prepare dataset and create dataloaders\n",
    "1. Evaluate pretrained MobileNetV2\n",
    "1. Calibrate, prune and evaluate pruned model equally\n",
    "1. Finetune and evaluate pruned model\n",
    "1. Usage of optimal pruning tool\n",
    "1. Finetune and evaluate optimal pruned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short summary for this notebook:\n",
    "\n",
    "``` python\n",
    "def loss_function(model_output, sample):\n",
    "    labels = sample[1].cuda()\n",
    "    return F.cross_entropy(model_output, labels, reduction='mean')\n",
    "\n",
    "def sample_to_n_samples(sample):\n",
    "    return sample[0].shape[0]\n",
    "\n",
    "def sample_to_model_inputs(sample):\n",
    "    images = sample[0].cuda()\n",
    "    return (images,), {}\n",
    "\n",
    "def latency_calculation_function(model, dataloader):\n",
    "    # Need to pass dataset items as input of network\n",
    "    inputs, _ = sample_to_model_inputs(next(iter(dataloader)))\n",
    "    ...\n",
    "    return model_latency\n",
    "\n",
    "\n",
    "lcf = partial(latency_calculation_function, dataloader=train_dataloader)\n",
    "\n",
    "# let's speed up model up to x3 times\n",
    "optimal_pruned_model = calibrate_and_prune_model_optimal(\n",
    "    model=baseline_model,\n",
    "    dataloader=train_dataloader,\n",
    "    loss_function=loss_function,\n",
    "    latency_calculation_function=lcf(baseline_model) / 3,\n",
    "    target_latency=desired_model_latency_value,\n",
    "    finetune_bn=True,\n",
    "    n_steps=None,\n",
    "    epochs=1,\n",
    "    n_network_runs=200,\n",
    "    sample_to_n_samples=sample_to_n_samples,\n",
    "    sample_to_model_inputs=sample_to_model_inputs,\n",
    "    verbose=2,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the environment\n",
    "\n",
    "First, let's set up the environment and make some common imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "# You may need to uncomment and change this variable to match free GPU index\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common:\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from tutorial_utils.checkpoints import download_imagenette_mobilenet\n",
    "from tutorial_utils.dataset import create_imagenette_dataloaders_for_pruning\n",
    "from tutorial_utils.train import accuracy\n",
    "\n",
    "# Training:\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch_optimizer import RAdam\n",
    "from tutorial_utils.phases import tutorial_train_loop\n",
    "from tutorial_utils.train import WarmupScheduler\n",
    "\n",
    "# Pruning:\n",
    "from enot.pruning import calibrate_and_prune_model_equal\n",
    "from enot.pruning import calibrate_and_prune_model_optimal\n",
    "\n",
    "# Latency:\n",
    "from fvcore.nn.flop_count import FlopCountAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model evaluation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function can evaluate both nn.Modules and executable functions.\n",
    "def eval_model(model, dataloader):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0.0\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader):\n",
    "\n",
    "            n = inputs.shape[0]\n",
    "\n",
    "            pred_labels = model(inputs)\n",
    "            batch_loss = criterion(pred_labels, labels)\n",
    "            batch_accuracy = accuracy(pred_labels, labels)\n",
    "\n",
    "            total += n\n",
    "            total_loss += batch_loss.item() * n\n",
    "            total_correct += batch_accuracy.item() * n\n",
    "\n",
    "    return total_loss / total, total_correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### In the following cell we setup all necessary dirs\n",
    "\n",
    "* `HOME_DIR` - experiments home directory\n",
    "* `DATASETS_DIR` - root directory for datasets (imagenette2, ...)\n",
    "* `PROJECT_DIR` - project directory to save training logs, checkpoints, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = Path.home() / '.optimization_experiments'\n",
    "DATASETS_DIR = HOME_DIR / 'datasets'\n",
    "PROJECT_DIR = HOME_DIR / 'e2e_pruning'\n",
    "\n",
    "HOME_DIR.mkdir(exist_ok=True)\n",
    "DATASETS_DIR.mkdir(exist_ok=True)\n",
    "PROJECT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset and create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, validation_dataloader = create_imagenette_dataloaders_for_pruning(\n",
    "    dataset_root_dir=DATASETS_DIR,\n",
    "    project_dir=PROJECT_DIR,\n",
    "    input_size=224,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate pretrained MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.mobilenetv2 import mobilenet_v2\n",
    "\n",
    "regular_model = mobilenet_v2(pretrained=False, num_classes=10).cuda()\n",
    "\n",
    "# Turning off FullyConnected layer dropout.\n",
    "# This is required to stabilize fine-tuning procedure.\n",
    "regular_model.classifier[0].p = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = PROJECT_DIR / 'e2e_imagenette_pruning.pth'\n",
    "download_imagenette_mobilenet(checkpoint_path)\n",
    "\n",
    "regular_model.load_state_dict(\n",
    "    torch.load(checkpoint_path)['model'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_accuracy = eval_model(regular_model, validation_dataloader)\n",
    "print(f'Regular (non-pruned) model: accuracy={val_accuracy:.3f}, loss={val_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrate, prune and evaluate pruned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define pruning ratio ``pruning_ratio`` (the amount of channels removed from the network), loss function ``loss_function`` (calculates mean loss for single batch of data loader), function to get the number of samples (images) in one data loader sample ``sample_to_n_samples``, and function to transform data loader sample to model inputs ``sample_to_model_inputs``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_ratio = 0.433  # This gives about x3 FLOPs reduction.\n",
    "\n",
    "\n",
    "def loss_function(model_output, sample):\n",
    "    labels = sample[1].cuda()\n",
    "    return F.cross_entropy(model_output, labels, reduction='mean')\n",
    "\n",
    "\n",
    "def sample_to_n_samples(sample):\n",
    "    return sample[0].shape[0]\n",
    "\n",
    "\n",
    "def sample_to_model_inputs(sample):\n",
    "    images = sample[0].cuda()\n",
    "    return (images,), {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform model pruning by calling ``calibrate_and_prune_model_equal``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pruned_model = calibrate_and_prune_model_equal(\n",
    "    model=regular_model,\n",
    "    dataloader=train_dataloader,\n",
    "    loss_function=loss_function,\n",
    "    pruning_ratio=pruning_ratio,\n",
    "    finetune_bn=True,\n",
    "    calibration_steps=None,  # When None - uses epochs argument to set the number of steps.\n",
    "    calibration_epochs=1,\n",
    "    sample_to_n_samples=sample_to_n_samples,\n",
    "    sample_to_model_inputs=sample_to_model_inputs,\n",
    "    show_tqdm=True,\n",
    ")\n",
    "pruned_model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pruning, pruned user model has the same structure as the original model, except that some convolutions, fully-connected layers and batch norm layers now have smaller number of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_accuracy = eval_model(pruned_model, validation_dataloader)\n",
    "print(f'Pruned model: accuracy={val_accuracy:.3f}, loss={val_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune and evaluate pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "N_WARMUP_EPOCHS = 1\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Uncomment lines below if you want to reach the best pruned model\n",
    "# performance (~93% accuracy for pruned model).\n",
    "\n",
    "# N_EPOCHS = 50  # Increase the number of model fine-tuning epochs.\n",
    "# N_WARMUP_EPOCHS = 10  # Increase the number of warmup epochs.\n",
    "# learning_rate = 0.01  # Increase learning rate\n",
    "\n",
    "len_train = len(train_dataloader)\n",
    "\n",
    "optimizer = RAdam(pruned_model.parameters(), lr=learning_rate, weight_decay=4e-5)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=len_train * N_EPOCHS)\n",
    "scheduler = WarmupScheduler(scheduler, warmup_steps=len_train * N_WARMUP_EPOCHS)\n",
    "loss = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "tutorial_train_loop(\n",
    "    epochs=N_EPOCHS,\n",
    "    model=pruned_model,\n",
    "    optimizer=optimizer,\n",
    "    metric_function=accuracy,\n",
    "    loss_function=loss,\n",
    "    train_loader=train_dataloader,\n",
    "    validation_loader=validation_dataloader,\n",
    "    scheduler=scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_accuracy = eval_model(pruned_model, validation_dataloader)\n",
    "print(f'Fine-tuned pruned model: accuracy={val_accuracy:.3f}, loss={val_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to find optimal model with same latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity we use MACs as latency function but feel free to use your own latency i.e.:\n",
    "1. CPU time\n",
    "2. GPU time\n",
    "3. Any other functions such as memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latency_calculation_function(model, dataloader):\n",
    "    # Need to pass dataset items as input of network\n",
    "    inputs, _ = sample_to_model_inputs(next(iter(dataloader)))\n",
    "    fca = FlopCountAnalysis(model=model.eval(), inputs=inputs)\n",
    "    fca.unsupported_ops_warnings(False)\n",
    "    fca.uncalled_modules_warnings(False)\n",
    "    original_mflops = fca.total() / 1e6\n",
    "    return original_mflops\n",
    "\n",
    "\n",
    "lcf = partial(latency_calculation_function, dataloader=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_latency = lcf(pruned_model.eval())\n",
    "\n",
    "print(f'Previously pruned model latency={desired_latency:.3f} MMACs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`latency_penalty` means how strong you restrict difference between target latency and searched model latency. Larger value leads to more precise match. \n",
    "\n",
    "If you need to find \"closest\" model to your desired latency you should increse this parameter. Since there is **latency/accuracy trade-off** you may fail to find optimal model in terms of quality. So you need to increase value of `n_search_steps`. This parameter enlarge total execution time of `calibrate_and_prune_model_optimal` function but leads to better searched model. If you fail to find model better than \"equal pruned\" try to increase `n_search_steps`.\n",
    "\n",
    "\n",
    "Note: if you use small pruning rates (up to x2 times) or your task is \"easy\" task, developers recomend to use   \n",
    "`calibrate_and_prune_model_equal` as best choice since almost every pruned model in such problem statement is OK. \n",
    "\n",
    "\n",
    "The default value of `n_search_steps` is 200, `latency_penalty` is 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'latency_penalty': 100,\n",
    "}\n",
    "\n",
    "optimal_pruned_model = calibrate_and_prune_model_optimal(\n",
    "    model=regular_model,\n",
    "    dataloader=train_dataloader,\n",
    "    loss_function=loss_function,\n",
    "    latency_calculation_function=lcf,\n",
    "    target_latency=desired_latency,\n",
    "    finetune_bn=True,\n",
    "    calibration_steps=None,  # When None - uses epochs argument to set the number of steps.\n",
    "    calibration_epochs=1,\n",
    "    n_search_steps=200,\n",
    "    sample_to_n_samples=sample_to_n_samples,\n",
    "    sample_to_model_inputs=sample_to_model_inputs,\n",
    "    show_tqdm=True,\n",
    "    **kwargs,\n",
    ")\n",
    "optimal_pruned_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_val_loss, opt_val_accuracy = eval_model(optimal_pruned_model, validation_dataloader)\n",
    "opt_lat = lcf(optimal_pruned_model.eval())\n",
    "\n",
    "print(f'Optimal pruned model: accuracy={opt_val_accuracy:.3f}, loss={opt_val_loss:.3f}, latency={opt_lat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "N_WARMUP_EPOCHS = 1\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Uncomment lines below if you want to reach the best pruned model\n",
    "# performance (~93% accuracy for pruned model).\n",
    "\n",
    "# N_EPOCHS = 50  # Increase the number of model fine-tuning epochs.\n",
    "# N_WARMUP_EPOCHS = 10  # Increase the number of warmup epochs.\n",
    "# learning_rate = 0.01  # Increase learning rate\n",
    "\n",
    "len_train = len(train_dataloader)\n",
    "\n",
    "optimizer = RAdam(optimal_pruned_model.parameters(), lr=learning_rate, weight_decay=4e-5)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=len_train * N_EPOCHS)\n",
    "scheduler = WarmupScheduler(scheduler, warmup_steps=len_train * N_WARMUP_EPOCHS)\n",
    "loss = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "tutorial_train_loop(\n",
    "    epochs=N_EPOCHS,\n",
    "    model=optimal_pruned_model,\n",
    "    optimizer=optimizer,\n",
    "    metric_function=accuracy,\n",
    "    loss_function=loss,\n",
    "    train_loader=train_dataloader,\n",
    "    validation_loader=validation_dataloader,\n",
    "    scheduler=scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 3000
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
