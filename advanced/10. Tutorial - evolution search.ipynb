{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with NAS\n",
    "\n",
    "This notebook describes the basic steps you need to optimize an architecture using NAS framework.\n",
    "\n",
    "### Main chapters of this notebook:\n",
    "1. Setup the environment\n",
    "1. Prepare dataset and create dataloaders\n",
    "1. Create model and move it into search space\n",
    "1. Pretrain constructed search space\n",
    "1. Search the best architecture\n",
    "1. Tune model with the best architecture\n",
    "1. Make model portable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the environment\n",
    "First, let's set up the environment and make some common imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "# You may need to uncomment and change this variable to match free GPU index\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch_optimizer import RAdam\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision.models.mobilenet import mobilenet_v2\n",
    "\n",
    "from enot.autogeneration import TransformationParameters\n",
    "from enot.autogeneration import generate_pruned_search_variants_model\n",
    "from enot.latency import current_latency\n",
    "from enot.latency import initialize_latency\n",
    "from enot.latency import min_latency\n",
    "from enot.latency import max_latency\n",
    "from enot.latency import mean_latency\n",
    "from enot.models import SearchSpaceModel\n",
    "from enot.optimize import FixedLatencySearchOptimizer\n",
    "from enot.optimize import PretrainOptimizer\n",
    "from enot.optimize.search import EvolutionSearch\n",
    "from enot.utils.benchmark import TorchBenchmarkRunner\n",
    "\n",
    "from tutorial_utils.checkpoints import download_autogen_pretrain_checkpoint\n",
    "from tutorial_utils.dataset import create_imagenette_dataloaders\n",
    "from tutorial_utils.phases import tutorial_train_loop\n",
    "from tutorial_utils.train import WarmupScheduler\n",
    "from tutorial_utils.train import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the following cell we setup all necessary dirs\n",
    "\n",
    "* `HOME_DIR` - experiments home directory\n",
    "* `DATASETS_DIR` - root directory for datasets (imagenette2, ...)\n",
    "* `PROJECT_DIR` - project directory to save training logs, checkpoints, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = Path.home() / '.optimization_experiments'\n",
    "DATASETS_DIR = HOME_DIR / 'datasets'\n",
    "PROJECT_DIR = HOME_DIR / 'getting_started'\n",
    "\n",
    "HOME_DIR.mkdir(exist_ok=True)\n",
    "DATASETS_DIR.mkdir(exist_ok=True)\n",
    "PROJECT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset and create dataloaders\n",
    "\n",
    "We will use dataset Imagenette2 in this example. <br>\n",
    "`create_imagenette_dataloaders` function prepares datasets for you in this example; specifically, it:\n",
    "1. downloads and unpacks dataset into `DATASETS_DIR`;\n",
    "1. splits dataset into 4 parts, and saves annotations of every part in `PROJECT_DIR` (We need this to prevent the overfitting effect, but for big datasets you can split dataset as usual train/val/test);\n",
    "1. creates dataloaders for every stage, and returns dataloaders as a dictionary.\n",
    "\n",
    "The four parts of the dataset:\n",
    "* train: for pretrain and tuning stages (`PROJECT_DIR`/train.csv)\n",
    "* validation: to choose checkpoint for architecture optimization (`PROJECT_DIR`/validation.csv)\n",
    "* search: for architecture search stage (`PROJECT_DIR`/search.csv)\n",
    "* test: hold-out data for testing (`PROJECT_DIR`/test.csv)\n",
    "\n",
    "Available dataloaders in the dictionary:\n",
    "* pretrain_train_dataloader:\n",
    "    training dataloader for \"pretrain\" stage (using data from `PROJECT_DIR`/train.csv)\n",
    "\n",
    "* pretrain_validation_dataloader:\n",
    "    validation dataloader for \"pretrain\" stage (using data from `PROJECT_DIR`/validation.csv)\n",
    "\n",
    "* search_train_dataloader:\n",
    "    training dataloader for \"search\" stage (using data from `PROJECT_DIR`/search.csv)\n",
    "\n",
    "* search_validation_dataloader:\n",
    "    validation dataloader for \"search\" stage (using data from `PROJECT_DIR`/search.csv)\n",
    "\n",
    "* tune_train_dataloader:\n",
    "    training dataloader for \"finetune\" stage (same as pretrain_train_dataloader)\n",
    "\n",
    "* tune_validation_dataloader:\n",
    "    validation dataloader for \"finetune\" stage (same as pretrain_validation_dataloader)\n",
    "\n",
    "**NOTE:**<br>\n",
    "CSV annotations use the following format:\n",
    "```\n",
    "filepath,label\n",
    "<relative_path_1>,<int_label_1>\n",
    "<relative_path_2>,<int_label_2>\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = create_imagenette_dataloaders(\n",
    "    dataset_root_dir=DATASETS_DIR,\n",
    "    project_dir=PROJECT_DIR,\n",
    "    input_size=(224, 224),\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model and move it into search space\n",
    "\n",
    "Our architecture optimization procedure selects the best combination of operations from a user-defined search space. The easiest way to define one is to take a base architecture and add similar operations with different parameters: kernel size, expansion ratio, activation, etc. You can also add different kinds of operations or implement your own (see <span style=\"color:green;white-space:nowrap\">***10. Tutorial - adding a custom operation for model builder***</span>).\n",
    "\n",
    "In this example, we took MobileNet v2 as a base architecture and made a search space from MobileNet inverted bottleneck blocks.\n",
    "\n",
    "First, let's define a model for an image classification task. MobileNet-like models can be built by `build_mobilenet` function. `build_mobilenet` function returns a model with the following structure: (inputs) -> stem -> blocks -> head -> (outputs). Stem and head have a fixed structure, while blocks consist of user-defined operations. The template class follows MobileNet v2 structure, which can be found in [torchvision library](https://github.com/pytorch/vision/blob/master/torchvision/models/mobilenetv2.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enot.models import SearchSpaceModel\n",
    "from enot.models.mobilenet import MobileNetBaseHead\n",
    "from enot.models.mobilenet import MobileNetBaseStem\n",
    "from enot.models.operations import SearchableMobileInvertedBottleneck\n",
    "from enot.models.operations import SearchableFuseableSkipConv\n",
    "from enot.models.operations import SearchVariantsContainer\n",
    "from enot.models.operations import SearchableFuseableSkipConv\n",
    "import torch.nn as nn\n",
    "\n",
    "# defining your model class\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stem = MobileNetBaseStem(in_channels=3)\n",
    "        self.body = nn.ModuleList(\n",
    "            [\n",
    "                # 3 blocks with 3 search options in each block\n",
    "                self.build_search_variants_1(16, 24),\n",
    "                # 2 fixed blocks\n",
    "                self.build_mib_k3_e6(24, 32, 2),\n",
    "                self.build_mib_k3_e6(32, 32, 1),\n",
    "                # 3 blocks with 3 search options in each block\n",
    "                self.build_search_variants_1(32, 64),\n",
    "                # 1 fixed block\n",
    "                self.build_mib_k3_e6(64, 96, 2),\n",
    "                # 2 blocks with 3 search options in each block\n",
    "                self.build_search_variants_1(96, 160),\n",
    "                # 1 block with 3 search options\n",
    "                self.build_search_variants_1(160, 320),\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = self.build_dropout_variants()\n",
    "        self.head = MobileNetBaseHead(\n",
    "            bottleneck_channels=320,\n",
    "            last_channels=1280,\n",
    "            num_classes=10,\n",
    "            dropout_rate=0.0,\n",
    "        )\n",
    "        head_list = list(self.head.head)\n",
    "        head_list.insert(3, self.dropout)\n",
    "        self.head.head = torch.nn.Sequential(*head_list)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_dropout_variants():\n",
    "        return SearchVariantsContainer(\n",
    "            [\n",
    "                torch.nn.Dropout(p=0.0),\n",
    "                torch.nn.Dropout(p=0.25),\n",
    "                torch.nn.Dropout(p=0.5),\n",
    "                torch.nn.Dropout(p=0.75),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def build_search_variants_1(in_channels, out_channels):\n",
    "        return SearchVariantsContainer(\n",
    "            [\n",
    "                SearchableMobileInvertedBottleneck(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    expand_ratio=6,\n",
    "                    padding=4,\n",
    "                    use_skip_connection=True,\n",
    "                    activation='relu',\n",
    "                ),\n",
    "                SearchableMobileInvertedBottleneck(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=5,\n",
    "                    stride=2,\n",
    "                    expand_ratio=6,\n",
    "                    padding=10,\n",
    "                    use_skip_connection=False,\n",
    "                    activation='relu6',\n",
    "                ),\n",
    "                SearchableMobileInvertedBottleneck(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=7,\n",
    "                    stride=1,\n",
    "                    expand_ratio=3,\n",
    "                    padding=12,\n",
    "                    use_skip_connection=True,\n",
    "                    activation='silu',\n",
    "                ),\n",
    "                SearchableFuseableSkipConv(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    stride=2,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def build_mib_k3_e6(in_channels, out_channels, stride):\n",
    "        return SearchableMobileInvertedBottleneck(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            expand_ratio=6,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "\n",
    "        for block in self.body:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "\n",
    "# move model to search space\n",
    "search_space = SearchSpaceModel(model).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain constructed search space\n",
    "\n",
    "\"Pretrain\" phase is the first phase of NAS procedure. In \"pretrain\" phase, we train user-defined search space with different architecture combinations to further compare their task performance in search phase. \n",
    "\n",
    "We offer `PretrainOptimizer` class (a wrapper for regular pytorch optimizer), which does all necessary \"pretrain\" magic. You should use `search_space` as the model and `PretrainOptimizer` as the optimizer in your train loop.\n",
    "\n",
    "Follow these steps to turn your train loop into enot \"pretrain\" loop:\n",
    "1. Pass `search_space.model_parameters()` to the optimizer instead of `search_space.parameters()`.\n",
    "1. Use `PretrainOptimizer` as model optimizer.\n",
    "1. Wrap model step with a closure, and send the closure into `pretrain_optimizer.step(...)` method as parameter. This is necessary because `PretrainOptimizer` does more than one step per batch. Alternatively, you can use `pretrain_optimizer.model_step(...)` in the combination with `pretrain_optimizer.step()` to make multiple gradient accumulations before the actual optimizer step.\n",
    "1. Call `search_space.sample_random_arch()` method on each validation step. This method samples a single architecture in the search space. This is necessary if you want measure the expectation of the search space performance, not the score of an individual random model.\n",
    "1. By default, `PretrainOptimizer` require one batch of train data to initialize optimizations, so you should run `search_space.initialize_output_distribution_optimization(...)` before first model step. You can disable optimization checking in `PretrainOptimizer` constructor (`check_recommended_optimizations` parameter), but it is not recomended.\n",
    "\n",
    "**IMPORTANT:**\n",
    "We set `N_EPOCHS`= 3 in this example to make tutorial execution faster. This is not enough for good pretrain quality, and you should set `N_EPOCHS`= 300 if you want to reproduce our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 3\n",
    "N_WARMUP_EPOCHS = 1\n",
    "\n",
    "train_loader = dataloaders['pretrain_train_dataloader']\n",
    "validation_loader = dataloaders['pretrain_validation_dataloader']\n",
    "\n",
    "# using `search_space.model_parameters()` as optimizable variables\n",
    "optimizer = SGD(params=search_space.model_parameters(), lr=0.06, momentum=0.9, weight_decay=1e-4)\n",
    "# using `PretrainOptimizer` as a default optimizer\n",
    "pretrain_optimizer = PretrainOptimizer(search_space=search_space, optimizer=optimizer)\n",
    "\n",
    "len_train_loader = len(train_loader)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=len_train_loader * N_EPOCHS, eta_min=1e-8)\n",
    "scheduler = WarmupScheduler(scheduler, warmup_steps=len_train_loader * N_WARMUP_EPOCHS)\n",
    "\n",
    "metric_function = accuracy\n",
    "loss_function = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    print(f'EPOCH #{epoch}')\n",
    "\n",
    "    search_space.train()\n",
    "    train_metrics_acc = {\n",
    "        'loss': 0.0,\n",
    "        'accuracy': 0.0,\n",
    "        'n': 0,\n",
    "    }\n",
    "    for inputs, labels in train_loader:\n",
    "        # By default, `PretrainOptimizer` requires one batch of train data to initialize optimizations,\n",
    "        # so you should run `search_space.initialize_output_distribution_optimization(...)` before the first\n",
    "        # model step. You can disable optimization checking in `PretrainOptimizer` constructor\n",
    "        # (`check_recommended_optimizations` parameter), but this is not recommended.\n",
    "        if not search_space.output_distribution_optimization_enabled:\n",
    "            search_space.initialize_output_distribution_optimization(inputs)\n",
    "\n",
    "        pretrain_optimizer.zero_grad()\n",
    "        # Wrapping model step and backward with closure.\n",
    "        # Alternatively, here is `pretrain_optimizer.model_step(...)` example usage for gradient accumulation:\n",
    "        #\n",
    "        # pretrain_optimizer.zero_grad()\n",
    "        # for inputs, labels in train_loader:\n",
    "        #\n",
    "        #     def closure():\n",
    "        #         ...\n",
    "        #\n",
    "        #     pretrain_optimizer.model_step(closure)\n",
    "        #     if (n + 1) % 10 == 0:\n",
    "        #         pretrain_optimizer.step()\n",
    "        #         pretrain_optimizer.zero_grad()\n",
    "        def closure():\n",
    "            pred_labels = search_space(inputs)\n",
    "            batch_loss = loss_function(pred_labels, labels)\n",
    "            batch_loss.backward()\n",
    "            batch_metric = metric_function(pred_labels, labels)\n",
    "\n",
    "            train_metrics_acc['loss'] += batch_loss.item()\n",
    "            train_metrics_acc['accuracy'] += batch_metric.item()\n",
    "            train_metrics_acc['n'] += 1\n",
    "\n",
    "        pretrain_optimizer.step(closure)\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    train_loss = train_metrics_acc['loss'] / train_metrics_acc['n']\n",
    "    train_accuracy = train_metrics_acc['accuracy'] / train_metrics_acc['n']\n",
    "\n",
    "    print('train metrics:')\n",
    "    print('  loss:', train_loss)\n",
    "    print('  accuracy:', train_accuracy)\n",
    "\n",
    "    # We are validating on one specific fixed architecture to avoid the effect of fluctuations\n",
    "    # in the values of the loss function and metrics.\n",
    "    arch_to_test = [0] * len(search_space.search_variants_containers)\n",
    "    test_model = search_space.get_network_by_indexes(arch_to_test)\n",
    "    test_model.eval()\n",
    "\n",
    "    validation_loss = 0\n",
    "    validation_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validation_loader:\n",
    "            pred_labels = test_model(inputs)\n",
    "            batch_loss = loss_function(pred_labels, labels)\n",
    "            batch_metric = metric_function(pred_labels, labels)\n",
    "\n",
    "            validation_loss += batch_loss.item()\n",
    "            validation_accuracy += batch_metric.item()\n",
    "\n",
    "    n = len(validation_loader)\n",
    "    validation_loss /= n\n",
    "    validation_accuracy /= n\n",
    "\n",
    "    print('validation metrics:')\n",
    "    print('  loss:', validation_loss)\n",
    "    print('  accuracy:', validation_accuracy)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search the best architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make function to evaluate optimized metric it can be metrics or loss\n",
    "def _evaluate(model, metric_function, dataloader):\n",
    "    metric = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        pred_labels = model(inputs)\n",
    "        batch_metric = metric_function(pred_labels, labels)\n",
    "        metric += batch_metric.item()\n",
    "\n",
    "    return metric / len(validation_loader)\n",
    "\n",
    "\n",
    "def evaluate_accuracy(model):\n",
    "    return _evaluate(\n",
    "        model=model,\n",
    "        metric_function=accuracy,\n",
    "        dataloader=dataloaders['search_train_dataloader'],\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_loss(model):\n",
    "    return -_evaluate(\n",
    "        model=model,\n",
    "        metric_function=loss_function,\n",
    "        dataloader=dataloaders['search_train_dataloader'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fvcore.nn.flop_count import FlopCountAnalysis\n",
    "\n",
    "latency_benchmark = TorchBenchmarkRunner(\n",
    "    benchmark_iterations=1000,\n",
    "    num_threads_per_process=2,\n",
    "    benchmark_max_time_s=3,\n",
    ")\n",
    "\n",
    "data_sample = next(iter(dataloaders['search_train_dataloader']))[0].cuda()\n",
    "search_space = search_space.cuda()\n",
    "\n",
    "\n",
    "def measure_model_latency(model):\n",
    "    return latency_benchmark(model, data_sample)[0]\n",
    "\n",
    "\n",
    "def measure_model_throughput(model):\n",
    "    return -latency_benchmark(model, data_sample)[-1]\n",
    "\n",
    "\n",
    "def measure_model_mmac(model):\n",
    "    # or search.space.forward_latency\n",
    "    fca = FlopCountAnalysis(model=model, inputs=data_sample)\n",
    "    fca.uncalled_modules_warnings(False)\n",
    "    fca.unsupported_ops_warnings(False)\n",
    "    return fca.total() / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find architecture with best accuracy without any latency constrains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_searcher = EvolutionSearch(\n",
    "    search_space=search_space,\n",
    "    evaluate_function=evaluate_accuracy,\n",
    "    search_steps=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_arch, best_acc = op_searcher.find_best_arch(return_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_arch, best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find architecture with maximal accuracy and defined mmac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space.sample(\n",
    "    [\n",
    "        [0],\n",
    "    ]\n",
    "    * len(search_space.search_variants_containers)\n",
    ")\n",
    "max_mmac = measure_model_mmac(search_space)\n",
    "\n",
    "search_space.sample(\n",
    "    [\n",
    "        [3],\n",
    "    ]\n",
    "    * len(search_space.search_variants_containers)\n",
    ")\n",
    "min_mmac = measure_model_mmac(search_space)\n",
    "print(max_mmac, min_mmac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_searcher = EvolutionSearch(\n",
    "    search_space=search_space,\n",
    "    evaluate_function=evaluate_accuracy,\n",
    "    calculate_latency_function=measure_model_mmac,\n",
    "    target_latency=1050,\n",
    "    search_steps=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_arch, best_acc, best_lat = op_searcher.find_best_arch(return_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_arch, best_acc, best_lat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find architecture with maximal accuracy and defined latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ss_n_cont = len(search_space.search_variants_containers)\n",
    "search_space.eval()\n",
    "with torch.no_grad():\n",
    "    search_space.sample(\n",
    "        [\n",
    "            [0],\n",
    "        ]\n",
    "        * ss_n_cont\n",
    "    )\n",
    "    print(measure_model_latency(search_space))\n",
    "\n",
    "    search_space.sample(\n",
    "        [\n",
    "            [3],\n",
    "        ]\n",
    "        * ss_n_cont\n",
    "    )\n",
    "    print(measure_model_latency(search_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_searcher = EvolutionSearch(\n",
    "    search_space=search_space,\n",
    "    evaluate_function=evaluate_accuracy,\n",
    "    target_latency=0.0023,\n",
    "    search_steps=20,\n",
    "    calculate_latency_function=measure_model_latency,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_arch, best_acc, best_lat = op_searcher.find_best_arch(return_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_arch, best_acc, best_lat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find architecture with minimal loss and defined throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_n_cont = len(search_space.search_variants_containers)\n",
    "search_space.eval()\n",
    "with torch.no_grad():\n",
    "    search_space.sample(\n",
    "        [\n",
    "            [0],\n",
    "        ]\n",
    "        * ss_n_cont\n",
    "    )\n",
    "    print(measure_model_throughput(search_space))\n",
    "\n",
    "    search_space.sample(\n",
    "        [\n",
    "            [3],\n",
    "        ]\n",
    "        * ss_n_cont\n",
    "    )\n",
    "    print(measure_model_throughput(search_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_searcher = EvolutionSearch(\n",
    "    search_space=search_space,\n",
    "    evaluate_function=evaluate_loss,\n",
    "    target_latency=-350,\n",
    "    search_steps=20,\n",
    "    calculate_latency_function=measure_model_throughput,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_arch, best_acc, best_lat = op_searcher.find_best_arch(return_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_arch, best_acc, best_lat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune model with the best architecture\n",
    "Now we take our best architecture from search space, and create a regular model using it. Then we run finetune procedure (usual training loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get regular model with the best architecture.\n",
    "best_model = search_space.get_network_by_indexes(best_arch).cuda()\n",
    "print(best_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "optimizer = SGD(best_model.parameters(), lr=2e-4, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = None\n",
    "metric_function = accuracy\n",
    "loss_function = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "train_loader = dataloaders['tune_train_dataloader']\n",
    "validation_loader = dataloaders['tune_validation_dataloader']\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    print(f'EPOCH #{epoch}')\n",
    "\n",
    "    best_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_labels = best_model(inputs)\n",
    "        batch_loss = loss_function(pred_labels, labels)\n",
    "        batch_loss.backward()\n",
    "        batch_metric = metric_function(pred_labels, labels)\n",
    "\n",
    "        train_loss += batch_loss.item()\n",
    "        train_accuracy += batch_metric.item()\n",
    "\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    n = len(train_loader)\n",
    "    train_loss /= n\n",
    "    train_accuracy /= n\n",
    "\n",
    "    print('train metrics:')\n",
    "    print('  loss:', train_loss)\n",
    "    print('  accuracy:', train_accuracy)\n",
    "\n",
    "    best_model.eval()\n",
    "    validation_loss = 0\n",
    "    validation_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validation_loader:\n",
    "            pred_labels = best_model(inputs)\n",
    "            batch_loss = loss_function(pred_labels, labels)\n",
    "            batch_metric = metric_function(pred_labels, labels)\n",
    "\n",
    "            validation_loss += batch_loss.item()\n",
    "            validation_accuracy += batch_metric.item()\n",
    "\n",
    "    n = len(validation_loader)\n",
    "    validation_loss /= n\n",
    "    validation_accuracy /= n\n",
    "\n",
    "    print('validation metrics:')\n",
    "    print('  loss:', validation_loss)\n",
    "    print('  accuracy:', validation_accuracy)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Make model portable\n",
    "Model will be free from our framework dependencies you can save it and load just with pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enot.utils.common import make_portable\n",
    "\n",
    "# Make sampled model free from our package\n",
    "portable_model = make_portable(best_model).cuda()\n",
    "\n",
    "# You can save it and load without our package just with pytorch\n",
    "model_path = PROJECT_DIR / 'best_model.pth'\n",
    "torch.save(portable_model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Reload kernel and run next cells only with torch dependent packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "\n",
    "from tutorial_utils.dataset import create_imagenette_dataloaders\n",
    "from tutorial_utils.train import accuracy\n",
    "\n",
    "\n",
    "HOME_DIR = Path.home() / '.optimization_experiments'\n",
    "DATASETS_DIR = HOME_DIR / 'datasets'\n",
    "PROJECT_DIR = HOME_DIR / 'getting_started'\n",
    "model_path = PROJECT_DIR / 'best_model.pth'\n",
    "\n",
    "metric_function = accuracy\n",
    "loss_function = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "dataloaders = create_imagenette_dataloaders(\n",
    "    dataset_root_dir=DATASETS_DIR,\n",
    "    project_dir=PROJECT_DIR,\n",
    "    input_size=(224, 224),\n",
    "    batch_size=32,\n",
    ")\n",
    "validation_loader = dataloaders['tune_validation_dataloader']\n",
    "\n",
    "# load saved portable model\n",
    "best_model_enot_free = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Best model works without our package!\n",
    "best_model_enot_free.eval()\n",
    "validation_loss = 0\n",
    "validation_accuracy = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in validation_loader:\n",
    "        pred_labels = best_model_enot_free(inputs)\n",
    "        batch_loss = loss_function(pred_labels, labels)\n",
    "        batch_metric = metric_function(pred_labels, labels)\n",
    "\n",
    "        validation_loss += batch_loss.item()\n",
    "        validation_accuracy += batch_metric.item()\n",
    "\n",
    "n = len(validation_loader)\n",
    "validation_loss /= n\n",
    "validation_accuracy /= n\n",
    "\n",
    "print('validation metrics:')\n",
    "print('  loss:', validation_loss)\n",
    "print('  accuracy:', validation_accuracy)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 3000
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
