{"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"language_info":{"file_extension":".py","version":"3.7.7","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3"},"notebookId":"8d8eba05-e17e-4bd2-83fe-accab3d13d02"},"cells":[{"cell_type":"markdown","source":"# Adding custom operations to ENOT\n\nIn this notebook we describe how you can implement your own operations to use with neural architecture search.\n\n### Notebook consists of next main stages:\n1. Setup the environment\n1. Add a custom operation to use with search space\n1. Build model with custom operation\n1. Check pretrain search and tune phases","metadata":{"cellId":"zpa9hauwgf1gnbkw41lgih"}},{"cell_type":"markdown","source":"## 1. Setup the environment\nFirst, let's set up the environment and common imports.","metadata":{"cellId":"nswku3xmm7kb637l5ee9"}},{"cell_type":"code","source":"#!L\nimport sys\n\nsys.path.append('ENOT_Tutorials')","metadata":{"cellId":"pnxx9jftzach33jgki7qg","trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"#!L\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\n\nfrom torch.optim import SGD\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch_optimizer import RAdam\n\nfrom enot.models import BaseSearchableOperation\nfrom enot.models import register_searchable_op\nfrom enot.models import SearchSpaceModel\nfrom enot.models.mobilenet import build_mobilenet\nfrom enot.phases import pretrain\nfrom enot.phases import search\nfrom enot.phases import train\n\nfrom enot_utils.metric_utils import accuracy\nfrom enot_utils.schedulers import WarmupScheduler\n\nfrom tutorial_utils.dataset import create_imagenette_dataloaders","metadata":{"cellId":"u27kcdzjq1xl5k4sn6ph","trusted":true},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## 2. Add a custom operation to use with search space\n\nWe have provided a bunch of pre-defined operations, but you can also implement your own.\n\nYou must follow next steps to create new searchable operation:\n1. Create new operation with base class `BaseSearchableOperation`\n    * The last layer of your operation should be Batch Normalization\n1. Initialize base class in you `__init__`. `BaseSearchableOperation` operation takes 4 required arguments:\n    * in_channels - number of channels in the input data\n    * out_channels - number of channels produced by the operation\n    * use_skip_connection - if you wanna apply skip connection set `True`, and `False` otherwise\n1. Implement abstract method `get_last_batch_norm`, which returns last Batch Normalization layer\n1. Implement abstract method `replace_last_batch_norm`, which replace last Batch Normalization layer by new one\n1. Implement abstract method `operation_forward`, which defines the computation performed at every call like common \"forward\", but ignore skip connection logic (skip connection logic implemented in \"forward\" method of base class) \n\nIf you wanna use custom operation with our model builders (`build_mobilenet`):\n1. They must be registered in our framework using `@register_searchable_op(name)`\n1. Operation must accept 4 required argument: in_channels, out_channels, stride, use_skip_connection. All other arguments must have default value or must be added in short config (see next paragraph).\n1. To use the short config format you must provide parameter parsing rules E. g. if you want to write `MyOp_k=3_t=6` instead of `{\"op_type\": \"MyOp\", \"kernel_size\": 3, \"expand_ratio\": 6.0}`, you need the following rules:\n```\n    {\n      'k': ('kernel_size', int),\n      't': ('expand_ratio', float)\n    }\n```","metadata":{"cellId":"b7k5dw1gl3jkdz7pr97u0m"}},{"cell_type":"code","source":"#!L\nactivations = {\n    'relu': nn.ReLU,\n    'sigmoid': nn.Sigmoid\n}\n# Define short parameter parsing rules\n# Format: {short_param_name: (original_param_name, parser)}\nshort_args = {\n    'k': ('kernel_size', int),\n    'a': ('activation', lambda x: activations[x])\n}\n\n\n@register_searchable_op('MyOp', short_args)\nclass MyOperation(BaseSearchableOperation):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        stride,\n        kernel_size=3,\n        activation=nn.ReLU,\n        padding=None,   \n        use_skip_connection=True\n    ):\n        super().__init__(\n            in_channels,\n            out_channels,\n            use_skip_connection,\n        )\n            \n        if padding is None:\n            padding = (kernel_size - 1) // 2\n        \n        self.operation = nn.Sequential(\n            nn.Conv2d(\n                in_channels=in_channels, \n                out_channels=out_channels, \n                kernel_size=kernel_size, \n                stride=stride, \n                padding=padding,\n            ),\n            activation(),\n            nn.BatchNorm2d(out_channels)\n        )\n\n    def get_last_batch_norm(self) -> nn.BatchNorm2d:\n        return self.operation[-1]\n\n    def replace_last_batch_norm(self, new_last_batch_norm: nn.BatchNorm2d) -> None:\n        self.operation[-1] = new_last_batch_norm\n        \n    def operation_forward(self, x):\n        return self.operation(x)","metadata":{"cellId":"3eswuyzstp1z1wfug2my8g","trusted":true},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## 3. Build model with custom operation","metadata":{"cellId":"qakd3din9qd2b4qkxer6wx"}},{"cell_type":"code","source":"#!L\nSEARCH_OPS = [\n    'MIB_k=3_t=6',\n    'MIB_k=5_t=6',\n    'MIB_k=7_t=6',\n    'MyOp_k=3',  # Notice that you can omit parameters with default values\n    'MyOp_k=3_a=sigmoid',\n]\n\n# build model\nmodel = build_mobilenet(\n    search_ops=SEARCH_OPS,\n    num_classes=10,\n    blocks_out_channels=[24, 32, 64, 96, 160, 320],\n    blocks_count=[2, 2, 2, 1, 2, 1],\n    blocks_stride=[2, 2, 2, 1, 2, 1],\n)\n# move model to search space\nsearch_space = SearchSpaceModel(model).cuda()","metadata":{"cellId":"6zqwwokqlec33ulxs0or8p","trusted":true},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"## 4. Check pretrain search and tune phases\n\nLet's check that everything works.","metadata":{"cellId":"y9b7xkjhznfw9sf7z20at"}},{"cell_type":"code","source":"#!L\nENOT_HOME_DIR = Path.home() / '.enot'\nENOT_DATASETS_DIR = ENOT_HOME_DIR / 'datasets'\nPROJECT_DIR = ENOT_HOME_DIR / 'custom_operation'\n\nENOT_HOME_DIR.mkdir(exist_ok=True)\nENOT_DATASETS_DIR.mkdir(exist_ok=True)\nPROJECT_DIR.mkdir(exist_ok=True)","metadata":{"cellId":"cjjqi46fbt22bjkqzdba4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\ndataloaders = create_imagenette_dataloaders(\n    dataset_root_dir=ENOT_DATASETS_DIR, \n    project_dir=PROJECT_DIR,\n    input_size=(224, 224),\n    batch_size=32,\n    imagenette_kind='imagenette2-320',\n)","metadata":{"cellId":"itxdb52yl8q0p1yqgwhmf7","trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"#!L\n# define directory for text logs and tensorboard logs\npretrain_dir = PROJECT_DIR / 'pretrain'\npretrain_dir.mkdir(exist_ok=True)\n\nN_EPOCHS = 3\nN_WARMUP_EPOCHS = 1\nlen_train = len(dataloaders['pretrain_train_dataloader'])\n\noptimizer = SGD(params=search_space.model_parameters(), lr=0.06, momentum=0.9, weight_decay=1e-4)\nscheduler = CosineAnnealingLR(optimizer, T_max=len_train*N_EPOCHS, eta_min=1e-8)\nscheduler = WarmupScheduler(scheduler, warmup_steps=len_train*N_WARMUP_EPOCHS)\nloss_function = nn.CrossEntropyLoss().cuda()\n\npretrain(\n    search_space=search_space,\n    exp_dir=pretrain_dir,\n    train_loader=dataloaders['pretrain_train_dataloader'],\n    valid_loader=dataloaders['pretrain_validation_dataloader'],\n    optimizer=optimizer,\n    scheduler=scheduler,\n    metric_function=accuracy,\n    loss_function=loss_function,\n    epochs=N_EPOCHS,\n)","metadata":{"scrolled":true,"cellId":"rpt0dh1xj1ebjlilu3rgrg","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\n# define directory for text logs and tensorboard logs\nsearch_dir = PROJECT_DIR / 'search'\nsearch_dir.mkdir(exist_ok=True)\n\noptimizer = RAdam(search_space.architecture_parameters(), lr=0.01)\n\nsearch(\n    search_space=search_space,\n    exp_dir=search_dir,\n    search_loader=dataloaders['search_train_dataloader'],\n    valid_loader=dataloaders['search_validation_dataloader'],\n    optimizer=optimizer,\n    loss_function=loss_function,\n    metric_function=accuracy,\n    epochs=3,\n)","metadata":{"cellId":"3uhhjcb7k5bw2121u5flti"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\n# get regular model with best architecture\nbest_model = search_space.get_network_with_best_arch().cuda()","metadata":{"cellId":"w4d5iivlmqn1odovw30ms"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\n# define directory for text logs and tensorboard logs\ntune_dir = PROJECT_DIR / 'tune'\ntune_dir.mkdir(exist_ok=True)\n\noptimizer = RAdam(best_model.parameters(), lr=5e-3, weight_decay=4e-5)\n\ntrain(\n    model=best_model,\n    exp_dir=tune_dir,\n    train_loader=dataloaders['tune_train_dataloader'],\n    valid_loader=dataloaders['tune_validation_dataloader'],\n    optimizer=optimizer,\n    loss_function=loss_function,\n    metric_function=accuracy,\n    epochs=3,\n)","metadata":{"cellId":"tkykedvo0xnga3h69yolq"},"outputs":[],"execution_count":null}]}